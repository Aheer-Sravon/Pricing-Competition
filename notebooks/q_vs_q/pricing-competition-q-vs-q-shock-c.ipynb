{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89e80aa1",
   "metadata": {
    "papermill": {
     "duration": 0.00443,
     "end_time": "2025-11-26T03:59:04.369575",
     "exception": false,
     "start_time": "2025-11-26T03:59:04.365145",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Imports and Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cb7a4de",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-26T03:59:04.378056Z",
     "iopub.status.busy": "2025-11-26T03:59:04.377655Z",
     "iopub.status.idle": "2025-11-26T03:59:11.405132Z",
     "shell.execute_reply": "2025-11-26T03:59:11.403990Z"
    },
    "papermill": {
     "duration": 7.033958,
     "end_time": "2025-11-26T03:59:11.407055",
     "exception": false,
     "start_time": "2025-11-26T03:59:04.373097",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Optional, Dict, Tuple\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "from scipy.optimize import minimize_scalar\n",
    "import pandas as pd\n",
    "\n",
    "SEED = 99\n",
    "NUM_RUNS = 50\n",
    "\n",
    "os.makedirs(\"results\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fddb0a",
   "metadata": {
    "papermill": {
     "duration": 0.003317,
     "end_time": "2025-11-26T03:59:11.413876",
     "exception": false,
     "start_time": "2025-11-26T03:59:11.410559",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# `TheoreticalBenchmarks`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ad26c42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T03:59:11.423092Z",
     "iopub.status.busy": "2025-11-26T03:59:11.422555Z",
     "iopub.status.idle": "2025-11-26T03:59:11.447792Z",
     "shell.execute_reply": "2025-11-26T03:59:11.446722Z"
    },
    "papermill": {
     "duration": 0.032102,
     "end_time": "2025-11-26T03:59:11.449466",
     "exception": false,
     "start_time": "2025-11-26T03:59:11.417364",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TheoreticalBenchmarks:\n",
    "    \"\"\"Calculate theoretical Nash and Monopoly prices/profits for market models\"\"\"\n",
    "    \n",
    "    def __init__(self, seed=None):\n",
    "        self.seed = seed\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "    \n",
    "    def calculate_logit_benchmarks(self, shock_cfg=None):\n",
    "        \"\"\"\n",
    "        Calculate Nash and Monopoly benchmarks for Logit model\n",
    "        With shocks: Uses Monte Carlo integration\n",
    "        Without shocks: Uses analytical formulas\n",
    "        \"\"\"\n",
    "        # Model parameters\n",
    "        a = 2.0\n",
    "        c = 1.0\n",
    "        mu = 0.25\n",
    "        a0 = 0.0\n",
    "        \n",
    "        if shock_cfg is None or not shock_cfg.get('enabled', False):\n",
    "            # No shocks - use standard benchmarks\n",
    "            p_N = 1.473\n",
    "            p_M = 1.925\n",
    "            \n",
    "            # Calculate profits at these prices\n",
    "            exp_N = np.exp((a - p_N) / mu)\n",
    "            exp_outside = np.exp(a0 / mu)\n",
    "            q_N = exp_N / (2 * exp_N + exp_outside)\n",
    "            E_pi_N = (p_N - c) * q_N\n",
    "            \n",
    "            exp_M = np.exp((a - p_M) / mu)\n",
    "            q_M = exp_M / (2 * exp_M + exp_outside)\n",
    "            E_pi_M = (p_M - c) * q_M\n",
    "            \n",
    "            return {\n",
    "                'p_N': p_N,\n",
    "                'E_pi_N': E_pi_N,\n",
    "                'p_M': p_M,\n",
    "                'E_pi_M': E_pi_M,\n",
    "                'shock_enabled': False\n",
    "            }\n",
    "        \n",
    "        # With shocks - use Monte Carlo\n",
    "        scheme_params = {\n",
    "            'A': {'rho': 0.3, 'sigma_eta': 0.5},\n",
    "            'B': {'rho': 0.95, 'sigma_eta': 0.05},\n",
    "            'C': {'rho': 0.9, 'sigma_eta': 0.3}\n",
    "        }\n",
    "        \n",
    "        scheme = shock_cfg.get('scheme', 'A')\n",
    "        params = scheme_params[scheme.upper()]\n",
    "        rho = params['rho']\n",
    "        sigma_eta = params['sigma_eta']\n",
    "        \n",
    "        # Unconditional variance of shocks\n",
    "        sigma2 = sigma_eta**2 / (1 - rho**2)\n",
    "        sigma = np.sqrt(sigma2)\n",
    "        \n",
    "        # Monte Carlo samples\n",
    "        N = 10000\n",
    "        shock_mode = shock_cfg.get('mode', 'independent')\n",
    "        \n",
    "        if shock_mode == 'independent':\n",
    "            xi_samples = np.random.normal(0, sigma, (N, 2))\n",
    "        else:  # correlated\n",
    "            xi_samples = np.random.normal(0, sigma, N)\n",
    "        \n",
    "        # Expected profit functions\n",
    "        def E_pi1(p1, p2):\n",
    "            if shock_mode == 'independent':\n",
    "                exps1 = np.exp((a - p1 + xi_samples[:, 0]) / mu)\n",
    "                exps2 = np.exp((a - p2 + xi_samples[:, 1]) / mu)\n",
    "            else:\n",
    "                exps1 = np.exp((a - p1 + xi_samples) / mu)\n",
    "                exps2 = np.exp((a - p2 + xi_samples) / mu)\n",
    "            den = exps1 + exps2 + np.exp(a0 / mu)\n",
    "            qs = exps1 / den\n",
    "            return np.mean((p1 - c) * qs)\n",
    "        \n",
    "        def E_pi2(p1, p2):\n",
    "            if shock_mode == 'independent':\n",
    "                exps1 = np.exp((a - p1 + xi_samples[:, 0]) / mu)\n",
    "                exps2 = np.exp((a - p2 + xi_samples[:, 1]) / mu)\n",
    "            else:\n",
    "                exps1 = np.exp((a - p1 + xi_samples) / mu)\n",
    "                exps2 = np.exp((a - p2 + xi_samples) / mu)\n",
    "            den = exps1 + exps2 + np.exp(a0 / mu)\n",
    "            qs = exps2 / den\n",
    "            return np.mean((p2 - c) * qs)\n",
    "        \n",
    "        # Nash equilibrium\n",
    "        def best_response(p_j):\n",
    "            def neg_E_pi(p_i):\n",
    "                return -E_pi1(p_i, p_j)\n",
    "            res = minimize_scalar(neg_E_pi, bounds=(c, c + 5), method='bounded')\n",
    "            return res.x\n",
    "        \n",
    "        p_guess = 1.5\n",
    "        for _ in range(50):\n",
    "            p_guess = best_response(p_guess)\n",
    "        p_N = p_guess\n",
    "        E_pi_N = E_pi1(p_N, p_N)\n",
    "        \n",
    "        # Monopoly\n",
    "        def neg_E_joint(p):\n",
    "            return -(E_pi1(p, p) + E_pi2(p, p))\n",
    "        \n",
    "        res_M = minimize_scalar(neg_E_joint, bounds=(c, c + 5), method='bounded')\n",
    "        p_M = res_M.x\n",
    "        E_pi_M = E_pi1(p_M, p_M)\n",
    "        \n",
    "        return {\n",
    "            'p_N': p_N,\n",
    "            'E_pi_N': E_pi_N,\n",
    "            'p_M': p_M,\n",
    "            'E_pi_M': E_pi_M,\n",
    "            'shock_enabled': True,\n",
    "            'scheme': scheme,\n",
    "            'mode': shock_mode,\n",
    "            'sigma': sigma\n",
    "        }\n",
    "    \n",
    "    def calculate_hotelling_benchmarks(self, shock_cfg=None):\n",
    "        \"\"\"\n",
    "        Calculate Nash and Monopoly benchmarks for Hotelling model\n",
    "        Shocks don't affect expected benchmarks (linearity in net shock)\n",
    "        \"\"\"\n",
    "        # Model parameters\n",
    "        c = 0.0\n",
    "        v_bar = 1.75\n",
    "        theta = 1.0\n",
    "        \n",
    "        # Standard benchmarks (unchanged with shocks)\n",
    "        p_N = 1.00\n",
    "        p_M = 1.25\n",
    "        \n",
    "        # Calculate profits\n",
    "        # At Nash: both firms charge 1, split market equally\n",
    "        q_N = 0.5\n",
    "        E_pi_N = p_N * q_N\n",
    "        \n",
    "        # At Monopoly: both charge 1.25, split market equally\n",
    "        q_M = 0.5\n",
    "        E_pi_M = p_M * q_M\n",
    "        \n",
    "        return {\n",
    "            'p_N': p_N,\n",
    "            'E_pi_N': E_pi_N,\n",
    "            'p_M': p_M,\n",
    "            'E_pi_M': E_pi_M,\n",
    "            'shock_enabled': shock_cfg is not None and shock_cfg.get('enabled', False),\n",
    "            'note': 'Shocks do not affect expected benchmarks for Hotelling'\n",
    "        }\n",
    "    \n",
    "    def calculate_linear_benchmarks(self, shock_cfg=None):\n",
    "        \"\"\"\n",
    "        Calculate Nash and Monopoly benchmarks for Linear model\n",
    "        Shocks don't affect expected benchmarks (linearity in shocks)\n",
    "        \"\"\"\n",
    "        # Model parameters\n",
    "        c = 0.0\n",
    "        a_bar = 1.0\n",
    "        d = 0.25\n",
    "        \n",
    "        # Standard benchmarks (unchanged with shocks)\n",
    "        p_N = 0.4286\n",
    "        p_M = 0.5\n",
    "        \n",
    "        # Calculate profits\n",
    "        denominator = 1 - d**2\n",
    "        \n",
    "        # At Nash\n",
    "        q_N = (a_bar - p_N - d * (a_bar - p_N)) / denominator\n",
    "        E_pi_N = p_N * q_N\n",
    "        \n",
    "        # At Monopoly\n",
    "        q_M = a_bar - p_M\n",
    "        E_pi_M = p_M * q_M\n",
    "        \n",
    "        return {\n",
    "            'p_N': p_N,\n",
    "            'E_pi_N': E_pi_N,\n",
    "            'p_M': p_M,\n",
    "            'E_pi_M': E_pi_M,\n",
    "            'shock_enabled': shock_cfg is not None and shock_cfg.get('enabled', False),\n",
    "            'note': 'Shocks do not affect expected benchmarks for Linear'\n",
    "        }\n",
    "    \n",
    "    def calculate_all_benchmarks(self, shock_cfg=None):\n",
    "        \"\"\"Calculate benchmarks for all three models\"\"\"\n",
    "        results = {\n",
    "            'logit': self.calculate_logit_benchmarks(shock_cfg),\n",
    "            'hotelling': self.calculate_hotelling_benchmarks(shock_cfg),\n",
    "            'linear': self.calculate_linear_benchmarks(shock_cfg)\n",
    "        }\n",
    "        return results\n",
    "    \n",
    "    def generate_benchmark_table(self, shock_configs):\n",
    "        \"\"\"\n",
    "        Generate a comprehensive table of benchmarks for different configurations\n",
    "        \n",
    "        Args:\n",
    "            shock_configs: List of shock configurations to test\n",
    "        \n",
    "        Returns:\n",
    "            pandas DataFrame with benchmarks\n",
    "        \"\"\"\n",
    "        data = []\n",
    "        \n",
    "        for config in shock_configs:\n",
    "            config_name = config.get('name', 'No Config')\n",
    "            benchmarks = self.calculate_all_benchmarks(config)\n",
    "            \n",
    "            for model, bench in benchmarks.items():\n",
    "                data.append({\n",
    "                    'Configuration': config_name,\n",
    "                    'Model': model.upper(),\n",
    "                    'Nash Price': round(bench['p_N'], 4),\n",
    "                    'Nash Profit': round(bench['E_pi_N'], 4),\n",
    "                    'Monopoly Price': round(bench['p_M'], 4),\n",
    "                    'Monopoly Profit': round(bench['E_pi_M'], 4),\n",
    "                    'Shock Enabled': bench['shock_enabled']\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e752be",
   "metadata": {
    "papermill": {
     "duration": 0.003181,
     "end_time": "2025-11-26T03:59:11.456041",
     "exception": false,
     "start_time": "2025-11-26T03:59:11.452860",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# `AR1_Shock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "906035b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T03:59:11.464124Z",
     "iopub.status.busy": "2025-11-26T03:59:11.463483Z",
     "iopub.status.idle": "2025-11-26T03:59:11.469212Z",
     "shell.execute_reply": "2025-11-26T03:59:11.468358Z"
    },
    "papermill": {
     "duration": 0.011439,
     "end_time": "2025-11-26T03:59:11.470668",
     "exception": false,
     "start_time": "2025-11-26T03:59:11.459229",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AR1_Shock:\n",
    "    def __init__(self, rho, sigma_eta, seed=None):\n",
    "        self.rho = rho\n",
    "        self.sigma_eta = sigma_eta\n",
    "        self.rng = np.random.RandomState(seed)\n",
    "        self.current = 0.0\n",
    "\n",
    "    def reset(self):\n",
    "        self.current = 0.0\n",
    "\n",
    "    def generate_next(self):\n",
    "        eta = self.rng.normal(0, self.sigma_eta)\n",
    "        self.current = self.rho * self.current + eta\n",
    "        return self.current\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef363c2",
   "metadata": {
    "papermill": {
     "duration": 0.00313,
     "end_time": "2025-11-26T03:59:11.477285",
     "exception": false,
     "start_time": "2025-11-26T03:59:11.474155",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# `MarketEnv` Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fd92285",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T03:59:11.485573Z",
     "iopub.status.busy": "2025-11-26T03:59:11.485129Z",
     "iopub.status.idle": "2025-11-26T03:59:11.629755Z",
     "shell.execute_reply": "2025-11-26T03:59:11.628698Z"
    },
    "papermill": {
     "duration": 0.151472,
     "end_time": "2025-11-26T03:59:11.631889",
     "exception": false,
     "start_time": "2025-11-26T03:59:11.480417",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MarketEnv:\n",
    "    \"\"\"Base Market environment supporting all three models with optional shocks\"\"\"\n",
    "   \n",
    "    def __init__(\n",
    "        self,\n",
    "        market_model: str = \"logit\",\n",
    "        shock_cfg: Optional[Dict] = None,\n",
    "        n_firms: int = 2,\n",
    "        horizon: int = 10000,\n",
    "        seed: Optional[int] = None\n",
    "    ):\n",
    "        self.model = market_model.lower()\n",
    "        self.n_firms = n_firms\n",
    "        self.N = 15 # Number of discrete prices\n",
    "        self.horizon = horizon\n",
    "        self.t = 0\n",
    "        self.rng = np.random.RandomState(seed)\n",
    "       \n",
    "        # Model-specific parameters (verified against PDFs)\n",
    "        if self.model == \"logit\":\n",
    "            self.c = 1.0\n",
    "            self.a = 2.0\n",
    "            self.a0 = 0.0\n",
    "            self.mu = 0.25\n",
    "            self.P_N = 1.473\n",
    "            self.P_M = 1.925\n",
    "        elif self.model == \"hotelling\":\n",
    "            self.c = 0.0\n",
    "            self.v_bar = 1.75\n",
    "            self.theta = 1.0\n",
    "            self.P_N = 1.00\n",
    "            self.P_M = 1.25\n",
    "        elif self.model == \"linear\":\n",
    "            self.c = 0.0\n",
    "            self.a_bar = 1.0\n",
    "            self.d = 0.25\n",
    "            \n",
    "            # Nash (Cournot duopoly)\n",
    "            self.P_N = self.a_bar * (1 - self.d) / (2 - self.d)  # 0.4286\n",
    "            q_N = self.P_N  # By symmetry at equilibrium\n",
    "            pi_N = self.P_N * q_N  # 0.1959\n",
    "            \n",
    "            # Monopoly (single firm, no competition)\n",
    "            self.P_M = self.a_bar / 2  # 0.5\n",
    "            q_M = self.a_bar - self.P_M  # 0.5 (residual demand, NOT Cournot!)\n",
    "            pi_M = self.P_M * q_M  # 0.25\n",
    "            \n",
    "            # Store for Delta calculations\n",
    "            self.pi_N_linear = pi_N  # 0.1959\n",
    "            self.pi_M_linear = pi_M  # 0.25\n",
    "            \n",
    "            # Verify profit range is healthy\n",
    "            profit_range = pi_M - pi_N  # Should be ~0.05\n",
    "            if profit_range < 0.04:\n",
    "                import warnings\n",
    "                warnings.warn(f\"Linear model profit range too small: {profit_range:.4f}\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model: {self.model}\")\n",
    "       \n",
    "        # Construct price grid\n",
    "        span = self.P_M - self.P_N\n",
    "        self.price_grid = np.linspace(\n",
    "            self.P_N - 0.15 * span,\n",
    "            self.P_M + 0.15 * span,\n",
    "            self.N\n",
    "        )\n",
    "       \n",
    "        # Initialize shock configuration\n",
    "        self.shock_cfg = shock_cfg or {}\n",
    "        self.shock_enabled = self.shock_cfg.get(\"enabled\", False)\n",
    "       \n",
    "        if self.shock_enabled:\n",
    "            self.shock_mode = self.shock_cfg.get(\"mode\", \"correlated\")\n",
    "            scheme = self.shock_cfg.get(\"scheme\", None)\n",
    "           \n",
    "            # Get AR(1) parameters (verified against PDFs)\n",
    "            if scheme:\n",
    "                scheme_params = {\n",
    "                    'A': {'rho': 0.3, 'sigma_eta': 0.5},\n",
    "                    'B': {'rho': 0.95, 'sigma_eta': 0.05},\n",
    "                    'C': {'rho': 0.9, 'sigma_eta': 0.3}\n",
    "                }\n",
    "                if scheme.upper() in scheme_params:\n",
    "                    params = scheme_params[scheme.upper()]\n",
    "                    self.rho = params['rho']\n",
    "                    self.sigma_eta = params['sigma_eta']\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown scheme: {scheme}\")\n",
    "            else:\n",
    "                self.rho = self.shock_cfg.get('rho', 0.9)\n",
    "                self.sigma_eta = self.shock_cfg.get('sigma_eta', 0.15)\n",
    "           \n",
    "            # Initialize shock generators\n",
    "            if self.shock_mode == \"independent\":\n",
    "                self.shock_generators = [\n",
    "                    AR1_Shock(self.rho, self.sigma_eta, seed=seed+i if seed else None)\n",
    "                    for i in range(self.n_firms)\n",
    "                ]\n",
    "            else: # correlated\n",
    "                self.shock_generator = AR1_Shock(self.rho, self.sigma_eta, seed=seed)\n",
    "           \n",
    "            self.current_shocks = np.zeros(self.n_firms)\n",
    "       \n",
    "        self.reset()\n",
    "   \n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment - returns state as actual prices\"\"\"\n",
    "        self.t = 0\n",
    "       \n",
    "        # Reset shocks\n",
    "        if self.shock_enabled:\n",
    "            if self.shock_mode == \"independent\":\n",
    "                for gen in self.shock_generators:\n",
    "                    gen.reset()\n",
    "            else:\n",
    "                self.shock_generator.reset()\n",
    "            self.current_shocks = np.zeros(self.n_firms)\n",
    "       \n",
    "        # Initialize prices at middle of grid\n",
    "        mid_idx = self.N // 2\n",
    "        self.current_price_idx = np.array([mid_idx] * self.n_firms)\n",
    "       \n",
    "        return self._get_state()\n",
    "   \n",
    "    def _get_state(self):\n",
    "        \"\"\"Get current state as actual prices (numpy array)\"\"\"\n",
    "        return self.price_grid[self.current_price_idx].copy()\n",
    "    \n",
    "    def _get_state_indices(self):\n",
    "        \"\"\"Get current state as price indices (for Q-learning compatibility)\"\"\"\n",
    "        return tuple(self.current_price_idx)\n",
    "   \n",
    "    def _generate_shocks(self):\n",
    "        \"\"\"Generate next period shocks\"\"\"\n",
    "        if not self.shock_enabled:\n",
    "            return np.zeros(self.n_firms)\n",
    "       \n",
    "        if self.shock_mode == \"independent\":\n",
    "            shocks = np.array([gen.generate_next() for gen in self.shock_generators])\n",
    "        else:\n",
    "            shock = self.shock_generator.generate_next()\n",
    "            shocks = np.array([shock] * self.n_firms)\n",
    "       \n",
    "        return shocks\n",
    "   \n",
    "    def get_current_shocks(self):\n",
    "        \"\"\"Get current shocks (for PSO evaluation)\"\"\"\n",
    "        return self.current_shocks.copy() if self.shock_enabled else np.zeros(self.n_firms)\n",
    "   \n",
    "    def calculate_demand_and_profit(self, prices: np.ndarray, shocks: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Calculate demand and profit given prices and shocks\"\"\"\n",
    "        if self.model == \"logit\":\n",
    "            return self._logit_demand_profit(prices, shocks)\n",
    "        elif self.model == \"hotelling\":\n",
    "            return self._hotelling_demand_profit(prices, shocks)\n",
    "        elif self.model == \"linear\":\n",
    "            return self._linear_demand_profit(prices, shocks)\n",
    "   \n",
    "    def _logit_demand_profit(self, prices: np.ndarray, shocks: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Logit demand - shocks affect variance but not Nash equilibrium\n",
    "        \n",
    "        Critical: E[demand | shocks] = demand(no shocks) since E[Îµ] = 0\n",
    "        Nash price must remain constant regardless of shock scheme\n",
    "        \"\"\"\n",
    "        # Base utilities WITHOUT shocks (defines Nash equilibrium)\n",
    "        base_utilities = (self.a - prices) / self.mu\n",
    "        \n",
    "        # Add shocks OUTSIDE mu scaling to preserve E[utility]\n",
    "        realized_utilities = base_utilities + shocks\n",
    "        \n",
    "        # Numerical stability\n",
    "        max_util = np.max(realized_utilities)\n",
    "        exp_utils = np.exp(realized_utilities - max_util)\n",
    "        exp_outside = np.exp(self.a0/self.mu - max_util)\n",
    "        \n",
    "        denominator = np.sum(exp_utils) + exp_outside\n",
    "        demands = exp_utils / denominator\n",
    "        profits = (prices - self.c) * demands\n",
    "        \n",
    "        return demands, profits\n",
    "   \n",
    "    def _hotelling_demand_profit(self, prices: np.ndarray, shocks: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Hotelling demand with net shock affecting boundary (verified against PDF)\"\"\"\n",
    "        p1, p2 = prices[0], prices[1]\n",
    "        epsilon_net = shocks[0] - shocks[1] if self.n_firms == 2 else 0\n",
    "       \n",
    "        x_hat = 0.5 + (p2 - p1 + epsilon_net) / (2 * self.theta)\n",
    "        x_hat = np.clip(x_hat, 0, 1)\n",
    "       \n",
    "        q1 = x_hat\n",
    "        q2 = 1 - x_hat\n",
    "        demands = np.array([q1, q2])\n",
    "        profits = prices * demands\n",
    "       \n",
    "        return demands, profits\n",
    "   \n",
    "    def _linear_demand_profit(self, prices: np.ndarray, shocks: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Linear demand with additive shocks (verified against PDF)\"\"\"\n",
    "        a_shocked = self.a_bar + shocks\n",
    "        denominator = 1 - self.d**2\n",
    "       \n",
    "        q1 = ((a_shocked[0] - prices[0]) - self.d * (a_shocked[1] - prices[1])) / denominator\n",
    "        q2 = ((a_shocked[1] - prices[1]) - self.d * (a_shocked[0] - prices[0])) / denominator\n",
    "       \n",
    "        q1 = max(0, q1)\n",
    "        q2 = max(0, q2)\n",
    "        demands = np.array([q1, q2])\n",
    "        profits = prices * demands\n",
    "       \n",
    "        return demands, profits\n",
    "   \n",
    "    def step(self, action_indices):\n",
    "        \"\"\"Execute one step - returns (state_prices, profits, done, info)\n",
    "        \n",
    "        Args:\n",
    "            action_indices: Array of price indices for each agent\n",
    "            \n",
    "        Returns:\n",
    "            next_state: Numpy array of actual prices (not indices)\n",
    "            profits: Numpy array of profits for each agent\n",
    "            done: Episode termination flag\n",
    "            info: Dictionary with prices, demands, and shocks\n",
    "        \"\"\"\n",
    "        action_indices = np.asarray(action_indices, dtype=int)\n",
    "       \n",
    "        # Update price indices\n",
    "        self.current_price_idx = action_indices\n",
    "        prices = self.price_grid[self.current_price_idx]\n",
    "       \n",
    "        # Generate shocks for this period\n",
    "        self.current_shocks = self._generate_shocks()\n",
    "       \n",
    "        # Calculate demands and profits\n",
    "        demands, profits = self.calculate_demand_and_profit(prices, self.current_shocks)\n",
    "       \n",
    "        # Update time\n",
    "        self.t += 1\n",
    "       \n",
    "        # Get next state (actual prices, not indices)\n",
    "        next_state = self._get_state()\n",
    "       \n",
    "        # Episode termination\n",
    "        done = False\n",
    "       \n",
    "        # Info dictionary\n",
    "        info = {\n",
    "            'prices': prices.copy(),\n",
    "            'demands': demands.copy(),\n",
    "            'shocks': self.current_shocks.copy(),\n",
    "            'price_indices': self.current_price_idx.copy()  # Include indices for Q-learning\n",
    "        }\n",
    "       \n",
    "        return next_state, profits, done, info\n",
    "\n",
    "class MarketEnvContinuous(MarketEnv):\n",
    "    \"\"\"Market environment that accepts both discrete indices and continuous prices as actions\"\"\"\n",
    "    \n",
    "    def step(self, actions):\n",
    "        \"\"\"Execute one step with flexible action handling\n",
    "        \n",
    "        Args:\n",
    "            actions: List/array where each element can be:\n",
    "                     - int/np.integer: discrete price index\n",
    "                     - float: continuous price value\n",
    "                     \n",
    "        Returns:\n",
    "            next_state: Numpy array of actual prices\n",
    "            profits: Numpy array of profits\n",
    "            done: Episode termination flag\n",
    "            info: Dictionary with prices, demands, shocks, and indices\n",
    "        \"\"\"\n",
    "        prices = []\n",
    "        indices = []\n",
    "        \n",
    "        for a in actions:\n",
    "            if isinstance(a, (int, np.integer)):\n",
    "                # Discrete action: use price from grid\n",
    "                prices.append(self.price_grid[a])\n",
    "                indices.append(a)\n",
    "            else:\n",
    "                # Continuous action: use actual price value\n",
    "                price = float(a)\n",
    "                prices.append(price)\n",
    "                # Find closest grid index for tracking\n",
    "                indices.append(np.argmin(np.abs(self.price_grid - price)))\n",
    "        \n",
    "        prices = np.array(prices)\n",
    "        self.current_price_idx = np.array(indices)\n",
    "        \n",
    "        # Generate shocks\n",
    "        self.current_shocks = self._generate_shocks()\n",
    "        \n",
    "        # Calculate demands and profits\n",
    "        demands, profits = self.calculate_demand_and_profit(prices, self.current_shocks)\n",
    "        \n",
    "        # Update time\n",
    "        self.t += 1\n",
    "        \n",
    "        # Get next state (actual prices)\n",
    "        next_state = self._get_state()\n",
    "        \n",
    "        # Episode termination\n",
    "        done = False\n",
    "        \n",
    "        # Info dictionary\n",
    "        info = {\n",
    "            'prices': prices.copy(),\n",
    "            'demands': demands.copy(),\n",
    "            'shocks': self.current_shocks.copy(),\n",
    "            'price_indices': self.current_price_idx.copy()\n",
    "        }\n",
    "        \n",
    "        return next_state, profits, done, info\n",
    "\n",
    "    def get_state_prices(self):\n",
    "        \"\"\"Get current state as actual prices (same as _get_state)\"\"\"\n",
    "        return self._get_state()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43042686",
   "metadata": {
    "papermill": {
     "duration": 0.003392,
     "end_time": "2025-11-26T03:59:11.638818",
     "exception": false,
     "start_time": "2025-11-26T03:59:11.635426",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# `QLearningAgent`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85dcd670",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T03:59:11.647254Z",
     "iopub.status.busy": "2025-11-26T03:59:11.646897Z",
     "iopub.status.idle": "2025-11-26T03:59:11.658368Z",
     "shell.execute_reply": "2025-11-26T03:59:11.657459Z"
    },
    "papermill": {
     "duration": 0.017892,
     "end_time": "2025-11-26T03:59:11.660183",
     "exception": false,
     "start_time": "2025-11-26T03:59:11.642291",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, n_actions, alpha=0.15, gamma=0.95, beta=1.5e-4, agent_id=0, price_grid=None):\n",
    "        \"\"\"\n",
    "        Q-Learning Agent for discrete pricing decisions.\n",
    "        \n",
    "        Args:\n",
    "            n_actions: Number of discrete price points\n",
    "            alpha: Learning rate\n",
    "            gamma: Discount factor\n",
    "            beta: Exploration decay rate\n",
    "            agent_id: Agent identifier (0 or 1)\n",
    "            price_grid: Price grid for converting continuous states to indices (optional)\n",
    "        \"\"\"\n",
    "        self.Q = np.zeros((n_actions, n_actions))\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.beta = beta  # Exploration decay rate\n",
    "        self.t = 0\n",
    "        self.id = agent_id\n",
    "        self.n_actions = n_actions\n",
    "        self.price_grid = price_grid  # Store price grid for state conversion\n",
    "    \n",
    "    def _state_to_indices(self, state):\n",
    "        \"\"\"Convert continuous state (prices) to discrete indices\n",
    "        \n",
    "        Args:\n",
    "            state: Can be either:\n",
    "                   - tuple of indices (old format, backward compatible)\n",
    "                   - numpy array of prices (new format)\n",
    "                   \n",
    "        Returns:\n",
    "            tuple of indices\n",
    "        \"\"\"\n",
    "        if isinstance(state, tuple):\n",
    "            # Already indices (backward compatible)\n",
    "            return state\n",
    "        elif isinstance(state, np.ndarray):\n",
    "            # Convert prices to indices\n",
    "            if self.price_grid is not None:\n",
    "                indices = tuple(np.argmin(np.abs(self.price_grid - price)) for price in state)\n",
    "                return indices\n",
    "            else:\n",
    "                # Fallback: assume state is small integers that can be indices\n",
    "                return tuple(state.astype(int))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown state format: {type(state)}\")\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"Choose action using epsilon-greedy with exponential decay\n",
    "        \n",
    "        Args:\n",
    "            state: Either tuple of indices or numpy array of prices\n",
    "            \n",
    "        Returns:\n",
    "            action: Discrete action index\n",
    "        \"\"\"\n",
    "        state_idx = self._state_to_indices(state)\n",
    "        opp_idx = state_idx[1 - self.id]\n",
    "        \n",
    "        epsilon = np.exp(-self.beta * self.t)\n",
    "        if np.random.rand() < epsilon:\n",
    "            return np.random.randint(self.n_actions)  # Explore\n",
    "        else:\n",
    "            return np.argmax(self.Q[opp_idx])  # Exploit\n",
    "\n",
    "    def update(self, state, action, reward, next_state):\n",
    "        \"\"\"Update Q-table using TD learning\n",
    "        \n",
    "        Args:\n",
    "            state: Current state (indices or prices)\n",
    "            action: Action taken\n",
    "            reward: Reward received\n",
    "            next_state: Next state (indices or prices)\n",
    "        \"\"\"\n",
    "        state_idx = self._state_to_indices(state)\n",
    "        next_state_idx = self._state_to_indices(next_state)\n",
    "        \n",
    "        opp_idx = state_idx[1 - self.id]\n",
    "        next_opp_idx = next_state_idx[1 - self.id]\n",
    "        \n",
    "        td_target = reward + self.gamma * np.max(self.Q[next_opp_idx])\n",
    "        self.Q[opp_idx, action] += self.alpha * (td_target - self.Q[opp_idx, action])\n",
    "        self.t += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b5bb7e",
   "metadata": {
    "papermill": {
     "duration": 0.003246,
     "end_time": "2025-11-26T03:59:11.666915",
     "exception": false,
     "start_time": "2025-11-26T03:59:11.663669",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e299898",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T03:59:11.675273Z",
     "iopub.status.busy": "2025-11-26T03:59:11.674819Z",
     "iopub.status.idle": "2025-11-26T04:02:51.837574Z",
     "shell.execute_reply": "2025-11-26T04:02:51.836378Z"
    },
    "papermill": {
     "duration": 220.169279,
     "end_time": "2025-11-26T04:02:51.839382",
     "exception": false,
     "start_time": "2025-11-26T03:59:11.670103",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Q-LEARNING vs Q-LEARNING - SCHEME C\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "Model: LOGIT\n",
      "============================================================\n",
      "\n",
      "Run 1:\n",
      "  Q-Agent 1 -> Delta: -3.6794, RPDI: -1.2726\n",
      "  Q-Agent 2 -> Delta: -4.0164, RPDI: -1.1224\n",
      "\n",
      "Run 2:\n",
      "  Q-Agent 1 -> Delta: -2.0685, RPDI: -0.8241\n",
      "  Q-Agent 2 -> Delta: -3.6650, RPDI: -0.7464\n",
      "\n",
      "Run 3:\n",
      "  Q-Agent 1 -> Delta: -4.1039, RPDI: -0.6920\n",
      "  Q-Agent 2 -> Delta: -1.8398, RPDI: -1.0824\n",
      "\n",
      "Run 4:\n",
      "  Q-Agent 1 -> Delta: -1.9921, RPDI: -0.8663\n",
      "  Q-Agent 2 -> Delta: -2.9202, RPDI: -0.7869\n",
      "\n",
      "Run 5:\n",
      "  Q-Agent 1 -> Delta: -3.9465, RPDI: -1.0390\n",
      "  Q-Agent 2 -> Delta: -2.4900, RPDI: -0.9809\n",
      "\n",
      "Run 6:\n",
      "  Q-Agent 1 -> Delta: -2.1056, RPDI: -0.9976\n",
      "  Q-Agent 2 -> Delta: -4.3381, RPDI: -0.9123\n",
      "\n",
      "Run 7:\n",
      "  Q-Agent 1 -> Delta: -3.7022, RPDI: -0.5220\n",
      "  Q-Agent 2 -> Delta: -1.7390, RPDI: -0.8353\n",
      "\n",
      "Run 8:\n",
      "  Q-Agent 1 -> Delta: -2.0856, RPDI: -0.8051\n",
      "  Q-Agent 2 -> Delta: -3.2975, RPDI: -0.6471\n",
      "\n",
      "Run 9:\n",
      "  Q-Agent 1 -> Delta: -3.0291, RPDI: -0.9855\n",
      "  Q-Agent 2 -> Delta: -3.5348, RPDI: -0.9248\n",
      "\n",
      "Run 10:\n",
      "  Q-Agent 1 -> Delta: -2.4678, RPDI: -0.9285\n",
      "  Q-Agent 2 -> Delta: -2.9482, RPDI: -0.7058\n",
      "\n",
      "Run 11:\n",
      "  Q-Agent 1 -> Delta: -2.7636, RPDI: -0.8045\n",
      "  Q-Agent 2 -> Delta: -1.8694, RPDI: -0.7409\n",
      "\n",
      "Run 12:\n",
      "  Q-Agent 1 -> Delta: -3.6598, RPDI: -0.7677\n",
      "  Q-Agent 2 -> Delta: -1.3924, RPDI: -0.9757\n",
      "\n",
      "Run 13:\n",
      "  Q-Agent 1 -> Delta: -1.8427, RPDI: -0.8842\n",
      "  Q-Agent 2 -> Delta: -3.6230, RPDI: -1.0016\n",
      "\n",
      "Run 14:\n",
      "  Q-Agent 1 -> Delta: -2.2489, RPDI: -0.8117\n",
      "  Q-Agent 2 -> Delta: -3.2731, RPDI: -0.8586\n",
      "\n",
      "Run 15:\n",
      "  Q-Agent 1 -> Delta: -3.1692, RPDI: -1.1397\n",
      "  Q-Agent 2 -> Delta: -4.0032, RPDI: -0.8519\n",
      "\n",
      "Run 16:\n",
      "  Q-Agent 1 -> Delta: -3.7517, RPDI: -0.7728\n",
      "  Q-Agent 2 -> Delta: -2.4115, RPDI: -0.9556\n",
      "\n",
      "Run 17:\n",
      "  Q-Agent 1 -> Delta: -2.8731, RPDI: -1.0729\n",
      "  Q-Agent 2 -> Delta: -3.6772, RPDI: -0.9098\n",
      "\n",
      "Run 18:\n",
      "  Q-Agent 1 -> Delta: -2.5577, RPDI: -0.9417\n",
      "  Q-Agent 2 -> Delta: -2.9153, RPDI: -0.8696\n",
      "\n",
      "Run 19:\n",
      "  Q-Agent 1 -> Delta: -2.9385, RPDI: -0.8798\n",
      "  Q-Agent 2 -> Delta: -2.7049, RPDI: -0.9995\n",
      "\n",
      "Run 20:\n",
      "  Q-Agent 1 -> Delta: -3.3076, RPDI: -0.9825\n",
      "  Q-Agent 2 -> Delta: -3.4496, RPDI: -1.0539\n",
      "\n",
      "Run 21:\n",
      "  Q-Agent 1 -> Delta: -4.4289, RPDI: -0.8103\n",
      "  Q-Agent 2 -> Delta: -1.8805, RPDI: -0.8985\n",
      "\n",
      "Run 22:\n",
      "  Q-Agent 1 -> Delta: -1.4885, RPDI: -0.8468\n",
      "  Q-Agent 2 -> Delta: -4.0176, RPDI: -0.9029\n",
      "\n",
      "Run 23:\n",
      "  Q-Agent 1 -> Delta: -3.0708, RPDI: -1.0272\n",
      "  Q-Agent 2 -> Delta: -3.3491, RPDI: -0.7764\n",
      "\n",
      "Run 24:\n",
      "  Q-Agent 1 -> Delta: -1.8038, RPDI: -0.9084\n",
      "  Q-Agent 2 -> Delta: -3.5865, RPDI: -0.7085\n",
      "\n",
      "Run 25:\n",
      "  Q-Agent 1 -> Delta: -3.3147, RPDI: -0.9262\n",
      "  Q-Agent 2 -> Delta: -3.3713, RPDI: -0.8675\n",
      "\n",
      "Run 26:\n",
      "  Q-Agent 1 -> Delta: -2.6690, RPDI: -0.9354\n",
      "  Q-Agent 2 -> Delta: -3.4220, RPDI: -0.8905\n",
      "\n",
      "Run 27:\n",
      "  Q-Agent 1 -> Delta: -3.7327, RPDI: -0.8362\n",
      "  Q-Agent 2 -> Delta: -2.3811, RPDI: -0.9503\n",
      "\n",
      "Run 28:\n",
      "  Q-Agent 1 -> Delta: -3.0343, RPDI: -1.1259\n",
      "  Q-Agent 2 -> Delta: -3.4558, RPDI: -0.9757\n",
      "\n",
      "Run 29:\n",
      "  Q-Agent 1 -> Delta: -2.6127, RPDI: -0.8555\n",
      "  Q-Agent 2 -> Delta: -2.9615, RPDI: -0.9613\n",
      "\n",
      "Run 30:\n",
      "  Q-Agent 1 -> Delta: -3.7880, RPDI: -1.0462\n",
      "  Q-Agent 2 -> Delta: -2.2024, RPDI: -0.8713\n",
      "\n",
      "Run 31:\n",
      "  Q-Agent 1 -> Delta: -2.1083, RPDI: -1.0686\n",
      "  Q-Agent 2 -> Delta: -3.9758, RPDI: -0.8733\n",
      "\n",
      "Run 32:\n",
      "  Q-Agent 1 -> Delta: -2.4315, RPDI: -0.7118\n",
      "  Q-Agent 2 -> Delta: -2.0341, RPDI: -0.7421\n",
      "\n",
      "Run 33:\n",
      "  Q-Agent 1 -> Delta: -1.9635, RPDI: -0.9172\n",
      "  Q-Agent 2 -> Delta: -3.7578, RPDI: -0.8571\n",
      "\n",
      "Run 34:\n",
      "  Q-Agent 1 -> Delta: -3.6625, RPDI: -1.1958\n",
      "  Q-Agent 2 -> Delta: -3.7214, RPDI: -1.0240\n",
      "\n",
      "Run 35:\n",
      "  Q-Agent 1 -> Delta: -2.4984, RPDI: -0.8387\n",
      "  Q-Agent 2 -> Delta: -2.9486, RPDI: -0.8082\n",
      "\n",
      "Run 36:\n",
      "  Q-Agent 1 -> Delta: -3.6129, RPDI: -0.9362\n",
      "  Q-Agent 2 -> Delta: -2.5503, RPDI: -1.0099\n",
      "\n",
      "Run 37:\n",
      "  Q-Agent 1 -> Delta: -3.1450, RPDI: -1.0707\n",
      "  Q-Agent 2 -> Delta: -4.1179, RPDI: -1.1069\n",
      "\n",
      "Run 38:\n",
      "  Q-Agent 1 -> Delta: -3.0491, RPDI: -0.9153\n",
      "  Q-Agent 2 -> Delta: -2.3254, RPDI: -0.8727\n",
      "\n",
      "Run 39:\n",
      "  Q-Agent 1 -> Delta: -4.0677, RPDI: -1.0220\n",
      "  Q-Agent 2 -> Delta: -3.1979, RPDI: -1.0530\n",
      "\n",
      "Run 40:\n",
      "  Q-Agent 1 -> Delta: -2.7052, RPDI: -0.7752\n",
      "  Q-Agent 2 -> Delta: -2.7611, RPDI: -0.9720\n",
      "\n",
      "Run 41:\n",
      "  Q-Agent 1 -> Delta: -2.9999, RPDI: -1.0874\n",
      "  Q-Agent 2 -> Delta: -4.1466, RPDI: -1.0005\n",
      "\n",
      "Run 42:\n",
      "  Q-Agent 1 -> Delta: -2.8246, RPDI: -0.8546\n",
      "  Q-Agent 2 -> Delta: -2.7097, RPDI: -0.7860\n",
      "\n",
      "Run 43:\n",
      "  Q-Agent 1 -> Delta: -3.0022, RPDI: -1.1086\n",
      "  Q-Agent 2 -> Delta: -3.1885, RPDI: -0.8384\n",
      "\n",
      "Run 44:\n",
      "  Q-Agent 1 -> Delta: -0.8321, RPDI: -0.9582\n",
      "  Q-Agent 2 -> Delta: -4.6529, RPDI: -0.5928\n",
      "\n",
      "Run 45:\n",
      "  Q-Agent 1 -> Delta: -3.9600, RPDI: -0.9585\n",
      "  Q-Agent 2 -> Delta: -3.3784, RPDI: -1.2629\n",
      "\n",
      "Run 46:\n",
      "  Q-Agent 1 -> Delta: -3.8148, RPDI: -0.7948\n",
      "  Q-Agent 2 -> Delta: -2.4779, RPDI: -0.9754\n",
      "\n",
      "Run 47:\n",
      "  Q-Agent 1 -> Delta: -3.0724, RPDI: -0.9785\n",
      "  Q-Agent 2 -> Delta: -2.7744, RPDI: -0.8581\n",
      "\n",
      "Run 48:\n",
      "  Q-Agent 1 -> Delta: -4.0581, RPDI: -0.8414\n",
      "  Q-Agent 2 -> Delta: -2.5797, RPDI: -1.0991\n",
      "\n",
      "Run 49:\n",
      "  Q-Agent 1 -> Delta: -3.5701, RPDI: -1.0484\n",
      "  Q-Agent 2 -> Delta: -3.7242, RPDI: -1.0697\n",
      "\n",
      "Run 50:\n",
      "  Q-Agent 1 -> Delta: -2.8105, RPDI: -0.9887\n",
      "  Q-Agent 2 -> Delta: -3.3582, RPDI: -0.9014\n",
      "\n",
      "============================================================\n",
      "Model: HOTELLING\n",
      "============================================================\n",
      "\n",
      "Run 1:\n",
      "  Q-Agent 1 -> Delta: 0.5132, RPDI: 0.5207\n",
      "  Q-Agent 2 -> Delta: 0.4200, RPDI: 0.5015\n",
      "\n",
      "Run 2:\n",
      "  Q-Agent 1 -> Delta: 0.9261, RPDI: 0.6386\n",
      "  Q-Agent 2 -> Delta: 0.3012, RPDI: 0.6274\n",
      "\n",
      "Run 3:\n",
      "  Q-Agent 1 -> Delta: 0.1083, RPDI: 0.5860\n",
      "  Q-Agent 2 -> Delta: 0.7081, RPDI: 0.3042\n",
      "\n",
      "Run 4:\n",
      "  Q-Agent 1 -> Delta: 0.8151, RPDI: 0.6071\n",
      "  Q-Agent 2 -> Delta: 0.2217, RPDI: 0.4706\n",
      "\n",
      "Run 5:\n",
      "  Q-Agent 1 -> Delta: -0.2387, RPDI: 0.4094\n",
      "  Q-Agent 2 -> Delta: 1.0710, RPDI: 0.4963\n",
      "\n",
      "Run 6:\n",
      "  Q-Agent 1 -> Delta: 1.2311, RPDI: 0.4538\n",
      "  Q-Agent 2 -> Delta: -0.3105, RPDI: 0.5877\n",
      "\n",
      "Run 7:\n",
      "  Q-Agent 1 -> Delta: 0.4562, RPDI: 0.5559\n",
      "  Q-Agent 2 -> Delta: 0.7062, RPDI: 0.6756\n",
      "\n",
      "Run 8:\n",
      "  Q-Agent 1 -> Delta: 0.6387, RPDI: 0.6549\n",
      "  Q-Agent 2 -> Delta: 0.4271, RPDI: 0.4681\n",
      "\n",
      "Run 9:\n",
      "  Q-Agent 1 -> Delta: 0.4323, RPDI: 0.5386\n",
      "  Q-Agent 2 -> Delta: 0.5339, RPDI: 0.4671\n",
      "\n",
      "Run 10:\n",
      "  Q-Agent 1 -> Delta: 0.2893, RPDI: 0.5603\n",
      "  Q-Agent 2 -> Delta: 0.8459, RPDI: 0.6397\n",
      "\n",
      "Run 11:\n",
      "  Q-Agent 1 -> Delta: 0.0202, RPDI: 0.4321\n",
      "  Q-Agent 2 -> Delta: 0.8814, RPDI: 0.5085\n",
      "\n",
      "Run 12:\n",
      "  Q-Agent 1 -> Delta: 0.1178, RPDI: 0.4494\n",
      "  Q-Agent 2 -> Delta: 0.8800, RPDI: 0.5642\n",
      "\n",
      "Run 13:\n",
      "  Q-Agent 1 -> Delta: 1.2816, RPDI: 0.5735\n",
      "  Q-Agent 2 -> Delta: -0.2904, RPDI: 0.4625\n",
      "\n",
      "Run 14:\n",
      "  Q-Agent 1 -> Delta: 0.8869, RPDI: 0.3506\n",
      "  Q-Agent 2 -> Delta: -0.1026, RPDI: 0.5019\n",
      "\n",
      "Run 15:\n",
      "  Q-Agent 1 -> Delta: 0.2196, RPDI: 0.3011\n",
      "  Q-Agent 2 -> Delta: 0.4895, RPDI: 0.4662\n",
      "\n",
      "Run 16:\n",
      "  Q-Agent 1 -> Delta: 0.2281, RPDI: 0.5467\n",
      "  Q-Agent 2 -> Delta: 0.7568, RPDI: 0.5114\n",
      "\n",
      "Run 17:\n",
      "  Q-Agent 1 -> Delta: 0.5064, RPDI: 0.4352\n",
      "  Q-Agent 2 -> Delta: 0.4321, RPDI: 0.5507\n",
      "\n",
      "Run 18:\n",
      "  Q-Agent 1 -> Delta: 0.6085, RPDI: 0.5425\n",
      "  Q-Agent 2 -> Delta: 0.4985, RPDI: 0.6148\n",
      "\n",
      "Run 19:\n",
      "  Q-Agent 1 -> Delta: 0.7339, RPDI: 0.5794\n",
      "  Q-Agent 2 -> Delta: 0.4134, RPDI: 0.6117\n",
      "\n",
      "Run 20:\n",
      "  Q-Agent 1 -> Delta: 0.7785, RPDI: 0.7782\n",
      "  Q-Agent 2 -> Delta: 0.5564, RPDI: 0.5934\n",
      "\n",
      "Run 21:\n",
      "  Q-Agent 1 -> Delta: -0.4328, RPDI: 0.6193\n",
      "  Q-Agent 2 -> Delta: 1.4868, RPDI: 0.5058\n",
      "\n",
      "Run 22:\n",
      "  Q-Agent 1 -> Delta: 1.3947, RPDI: 0.4668\n",
      "  Q-Agent 2 -> Delta: -0.5242, RPDI: 0.4624\n",
      "\n",
      "Run 23:\n",
      "  Q-Agent 1 -> Delta: 0.2309, RPDI: 0.5250\n",
      "  Q-Agent 2 -> Delta: 1.0412, RPDI: 0.7866\n",
      "\n",
      "Run 24:\n",
      "  Q-Agent 1 -> Delta: 0.8302, RPDI: 0.4005\n",
      "  Q-Agent 2 -> Delta: -0.0305, RPDI: 0.4487\n",
      "\n",
      "Run 25:\n",
      "  Q-Agent 1 -> Delta: 0.1426, RPDI: 0.4008\n",
      "  Q-Agent 2 -> Delta: 0.5533, RPDI: 0.3576\n",
      "\n",
      "Run 26:\n",
      "  Q-Agent 1 -> Delta: 0.9381, RPDI: 0.5202\n",
      "  Q-Agent 2 -> Delta: 0.0796, RPDI: 0.5549\n",
      "\n",
      "Run 27:\n",
      "  Q-Agent 1 -> Delta: 0.0376, RPDI: 0.5140\n",
      "  Q-Agent 2 -> Delta: 0.7990, RPDI: 0.3883\n",
      "\n",
      "Run 28:\n",
      "  Q-Agent 1 -> Delta: 0.2569, RPDI: 0.4265\n",
      "  Q-Agent 2 -> Delta: 0.3803, RPDI: 0.2577\n",
      "\n",
      "Run 29:\n",
      "  Q-Agent 1 -> Delta: 0.7621, RPDI: 0.4453\n",
      "  Q-Agent 2 -> Delta: 0.1130, RPDI: 0.4883\n",
      "\n",
      "Run 30:\n",
      "  Q-Agent 1 -> Delta: -0.4607, RPDI: 0.4535\n",
      "  Q-Agent 2 -> Delta: 1.4567, RPDI: 0.5539\n",
      "\n",
      "Run 31:\n",
      "  Q-Agent 1 -> Delta: 0.9339, RPDI: 0.4394\n",
      "  Q-Agent 2 -> Delta: -0.0741, RPDI: 0.4895\n",
      "\n",
      "Run 32:\n",
      "  Q-Agent 1 -> Delta: 0.2932, RPDI: 0.5206\n",
      "  Q-Agent 2 -> Delta: 0.5960, RPDI: 0.4190\n",
      "\n",
      "Run 33:\n",
      "  Q-Agent 1 -> Delta: 1.1343, RPDI: 0.6204\n",
      "  Q-Agent 2 -> Delta: 0.0095, RPDI: 0.5693\n",
      "\n",
      "Run 34:\n",
      "  Q-Agent 1 -> Delta: 0.0362, RPDI: 0.4493\n",
      "  Q-Agent 2 -> Delta: 0.6328, RPDI: 0.3030\n",
      "\n",
      "Run 35:\n",
      "  Q-Agent 1 -> Delta: 0.6810, RPDI: 0.5583\n",
      "  Q-Agent 2 -> Delta: 0.3609, RPDI: 0.5480\n",
      "\n",
      "Run 36:\n",
      "  Q-Agent 1 -> Delta: 0.2548, RPDI: 0.5737\n",
      "  Q-Agent 2 -> Delta: 0.7913, RPDI: 0.5358\n",
      "\n",
      "Run 37:\n",
      "  Q-Agent 1 -> Delta: 0.9816, RPDI: 0.6127\n",
      "  Q-Agent 2 -> Delta: 0.1138, RPDI: 0.5148\n",
      "\n",
      "Run 38:\n",
      "  Q-Agent 1 -> Delta: 0.1365, RPDI: 0.4404\n",
      "  Q-Agent 2 -> Delta: 0.8286, RPDI: 0.5542\n",
      "\n",
      "Run 39:\n",
      "  Q-Agent 1 -> Delta: 0.2885, RPDI: 0.4245\n",
      "  Q-Agent 2 -> Delta: 0.6688, RPDI: 0.5790\n",
      "\n",
      "Run 40:\n",
      "  Q-Agent 1 -> Delta: 0.8065, RPDI: 0.4707\n",
      "  Q-Agent 2 -> Delta: 0.1807, RPDI: 0.5926\n",
      "\n",
      "Run 41:\n",
      "  Q-Agent 1 -> Delta: 0.8109, RPDI: 0.5215\n",
      "  Q-Agent 2 -> Delta: 0.1771, RPDI: 0.5006\n",
      "\n",
      "Run 42:\n",
      "  Q-Agent 1 -> Delta: 0.4111, RPDI: 0.5605\n",
      "  Q-Agent 2 -> Delta: 0.7688, RPDI: 0.6686\n",
      "\n",
      "Run 43:\n",
      "  Q-Agent 1 -> Delta: 0.0550, RPDI: 0.5835\n",
      "  Q-Agent 2 -> Delta: 1.0711, RPDI: 0.5916\n",
      "\n",
      "Run 44:\n",
      "  Q-Agent 1 -> Delta: 1.1296, RPDI: 0.4513\n",
      "  Q-Agent 2 -> Delta: -0.2255, RPDI: 0.5252\n",
      "\n",
      "Run 45:\n",
      "  Q-Agent 1 -> Delta: 0.6471, RPDI: 0.5512\n",
      "  Q-Agent 2 -> Delta: 0.3451, RPDI: 0.5167\n",
      "\n",
      "Run 46:\n",
      "  Q-Agent 1 -> Delta: 0.3330, RPDI: 0.5699\n",
      "  Q-Agent 2 -> Delta: 0.6677, RPDI: 0.4864\n",
      "\n",
      "Run 47:\n",
      "  Q-Agent 1 -> Delta: 0.3173, RPDI: 0.4052\n",
      "  Q-Agent 2 -> Delta: 0.6898, RPDI: 0.6530\n",
      "\n",
      "Run 48:\n",
      "  Q-Agent 1 -> Delta: 0.2820, RPDI: 0.3935\n",
      "  Q-Agent 2 -> Delta: 0.4453, RPDI: 0.3875\n",
      "\n",
      "Run 49:\n",
      "  Q-Agent 1 -> Delta: 0.5956, RPDI: 0.3187\n",
      "  Q-Agent 2 -> Delta: 0.1473, RPDI: 0.4760\n",
      "\n",
      "Run 50:\n",
      "  Q-Agent 1 -> Delta: 0.5446, RPDI: 0.5732\n",
      "  Q-Agent 2 -> Delta: 0.4431, RPDI: 0.4902\n",
      "\n",
      "============================================================\n",
      "Model: LINEAR\n",
      "============================================================\n",
      "\n",
      "Run 1:\n",
      "  Q-Agent 1 -> Delta: 1.7666, RPDI: 0.6860\n",
      "  Q-Agent 2 -> Delta: 1.6823, RPDI: 0.6898\n",
      "\n",
      "Run 2:\n",
      "  Q-Agent 1 -> Delta: 1.8684, RPDI: 0.5628\n",
      "  Q-Agent 2 -> Delta: 1.1543, RPDI: 0.6302\n",
      "\n",
      "Run 3:\n",
      "  Q-Agent 1 -> Delta: 1.3070, RPDI: 0.5626\n",
      "  Q-Agent 2 -> Delta: 1.6522, RPDI: 0.4963\n",
      "\n",
      "Run 4:\n",
      "  Q-Agent 1 -> Delta: 1.7370, RPDI: 0.5477\n",
      "  Q-Agent 2 -> Delta: 1.2313, RPDI: 0.4435\n",
      "\n",
      "Run 5:\n",
      "  Q-Agent 1 -> Delta: 1.0952, RPDI: 0.6859\n",
      "  Q-Agent 2 -> Delta: 2.6280, RPDI: 0.3867\n",
      "\n",
      "Run 6:\n",
      "  Q-Agent 1 -> Delta: 2.6270, RPDI: 0.6557\n",
      "  Q-Agent 2 -> Delta: 0.7875, RPDI: 0.5554\n",
      "\n",
      "Run 7:\n",
      "  Q-Agent 1 -> Delta: 1.0119, RPDI: 0.3300\n",
      "  Q-Agent 2 -> Delta: 1.6751, RPDI: 0.5581\n",
      "\n",
      "Run 8:\n",
      "  Q-Agent 1 -> Delta: 1.7228, RPDI: 0.6841\n",
      "  Q-Agent 2 -> Delta: 1.0562, RPDI: 0.4841\n",
      "\n",
      "Run 9:\n",
      "  Q-Agent 1 -> Delta: 1.1697, RPDI: 0.6469\n",
      "  Q-Agent 2 -> Delta: 0.9814, RPDI: 0.4326\n",
      "\n",
      "Run 10:\n",
      "  Q-Agent 1 -> Delta: 0.9389, RPDI: 0.6855\n",
      "  Q-Agent 2 -> Delta: 1.5377, RPDI: 0.7632\n",
      "\n",
      "Run 11:\n",
      "  Q-Agent 1 -> Delta: 1.1516, RPDI: 0.4600\n",
      "  Q-Agent 2 -> Delta: 2.2974, RPDI: 0.5728\n",
      "\n",
      "Run 12:\n",
      "  Q-Agent 1 -> Delta: 1.7514, RPDI: 0.5218\n",
      "  Q-Agent 2 -> Delta: 3.0701, RPDI: 0.5267\n",
      "\n",
      "Run 13:\n",
      "  Q-Agent 1 -> Delta: 3.3513, RPDI: 0.6551\n",
      "  Q-Agent 2 -> Delta: 0.9789, RPDI: 0.4673\n",
      "\n",
      "Run 14:\n",
      "  Q-Agent 1 -> Delta: 1.6777, RPDI: 0.5188\n",
      "  Q-Agent 2 -> Delta: 1.0379, RPDI: 0.5856\n",
      "\n",
      "Run 15:\n",
      "  Q-Agent 1 -> Delta: 1.0844, RPDI: 0.4568\n",
      "  Q-Agent 2 -> Delta: 1.0560, RPDI: 0.4455\n",
      "\n",
      "Run 16:\n",
      "  Q-Agent 1 -> Delta: 0.8745, RPDI: 0.4946\n",
      "  Q-Agent 2 -> Delta: 1.6153, RPDI: 0.5568\n",
      "\n",
      "Run 17:\n",
      "  Q-Agent 1 -> Delta: 1.4880, RPDI: 0.3661\n",
      "  Q-Agent 2 -> Delta: 1.5284, RPDI: 0.5107\n",
      "\n",
      "Run 18:\n",
      "  Q-Agent 1 -> Delta: 1.5321, RPDI: 0.5428\n",
      "  Q-Agent 2 -> Delta: 1.3943, RPDI: 0.5054\n",
      "\n",
      "Run 19:\n",
      "  Q-Agent 1 -> Delta: 1.4302, RPDI: 0.5913\n",
      "  Q-Agent 2 -> Delta: 1.1154, RPDI: 0.5012\n",
      "\n",
      "Run 20:\n",
      "  Q-Agent 1 -> Delta: 1.3139, RPDI: 0.4603\n",
      "  Q-Agent 2 -> Delta: 1.0069, RPDI: 0.2756\n",
      "\n",
      "Run 21:\n",
      "  Q-Agent 1 -> Delta: 0.5654, RPDI: 0.5802\n",
      "  Q-Agent 2 -> Delta: 2.8496, RPDI: 0.5732\n",
      "\n",
      "Run 22:\n",
      "  Q-Agent 1 -> Delta: 2.9947, RPDI: 0.5436\n",
      "  Q-Agent 2 -> Delta: 0.3683, RPDI: 0.4926\n",
      "\n",
      "Run 23:\n",
      "  Q-Agent 1 -> Delta: 0.6755, RPDI: 0.4796\n",
      "  Q-Agent 2 -> Delta: 1.5670, RPDI: 0.7011\n",
      "\n",
      "Run 24:\n",
      "  Q-Agent 1 -> Delta: 1.5810, RPDI: 0.5193\n",
      "  Q-Agent 2 -> Delta: 0.7998, RPDI: 0.6363\n",
      "\n",
      "Run 25:\n",
      "  Q-Agent 1 -> Delta: 0.6306, RPDI: 0.5071\n",
      "  Q-Agent 2 -> Delta: 0.7597, RPDI: 0.4249\n",
      "\n",
      "Run 26:\n",
      "  Q-Agent 1 -> Delta: 1.0975, RPDI: 0.6987\n",
      "  Q-Agent 2 -> Delta: 0.5099, RPDI: 0.5804\n",
      "\n",
      "Run 27:\n",
      "  Q-Agent 1 -> Delta: 0.4800, RPDI: 0.6090\n",
      "  Q-Agent 2 -> Delta: 1.3141, RPDI: 0.3644\n",
      "\n",
      "Run 28:\n",
      "  Q-Agent 1 -> Delta: 1.1562, RPDI: 0.5150\n",
      "  Q-Agent 2 -> Delta: 1.2463, RPDI: 0.4357\n",
      "\n",
      "Run 29:\n",
      "  Q-Agent 1 -> Delta: 1.3201, RPDI: 0.4186\n",
      "  Q-Agent 2 -> Delta: 0.7375, RPDI: 0.4860\n",
      "\n",
      "Run 30:\n",
      "  Q-Agent 1 -> Delta: 0.5566, RPDI: 0.6789\n",
      "  Q-Agent 2 -> Delta: 2.8687, RPDI: 0.5656\n",
      "\n",
      "Run 31:\n",
      "  Q-Agent 1 -> Delta: 2.4695, RPDI: 0.4315\n",
      "  Q-Agent 2 -> Delta: 1.1217, RPDI: 0.5129\n",
      "\n",
      "Run 32:\n",
      "  Q-Agent 1 -> Delta: 1.3407, RPDI: 0.4601\n",
      "  Q-Agent 2 -> Delta: 1.6642, RPDI: 0.3832\n",
      "\n",
      "Run 33:\n",
      "  Q-Agent 1 -> Delta: 1.9587, RPDI: 0.3553\n",
      "  Q-Agent 2 -> Delta: 0.4800, RPDI: 0.5772\n",
      "\n",
      "Run 34:\n",
      "  Q-Agent 1 -> Delta: 0.5924, RPDI: 0.4233\n",
      "  Q-Agent 2 -> Delta: 1.3159, RPDI: 0.4870\n",
      "\n",
      "Run 35:\n",
      "  Q-Agent 1 -> Delta: 1.2752, RPDI: 0.5854\n",
      "  Q-Agent 2 -> Delta: 1.0762, RPDI: 0.6750\n",
      "\n",
      "Run 36:\n",
      "  Q-Agent 1 -> Delta: 0.9384, RPDI: 0.5318\n",
      "  Q-Agent 2 -> Delta: 1.5637, RPDI: 0.6597\n",
      "\n",
      "Run 37:\n",
      "  Q-Agent 1 -> Delta: 1.5929, RPDI: 0.5213\n",
      "  Q-Agent 2 -> Delta: 0.3639, RPDI: 0.4769\n",
      "\n",
      "Run 38:\n",
      "  Q-Agent 1 -> Delta: 0.4959, RPDI: 0.5219\n",
      "  Q-Agent 2 -> Delta: 1.4083, RPDI: 0.6825\n",
      "\n",
      "Run 39:\n",
      "  Q-Agent 1 -> Delta: 0.9841, RPDI: 0.4122\n",
      "  Q-Agent 2 -> Delta: 1.8312, RPDI: 0.6210\n",
      "\n",
      "Run 40:\n",
      "  Q-Agent 1 -> Delta: 1.7947, RPDI: 0.3524\n",
      "  Q-Agent 2 -> Delta: 1.0180, RPDI: 0.5956\n",
      "\n",
      "Run 41:\n",
      "  Q-Agent 1 -> Delta: 1.1233, RPDI: 0.2802\n",
      "  Q-Agent 2 -> Delta: 0.3369, RPDI: 0.5863\n",
      "\n",
      "Run 42:\n",
      "  Q-Agent 1 -> Delta: 0.3824, RPDI: 0.5853\n",
      "  Q-Agent 2 -> Delta: 1.1674, RPDI: 0.5126\n",
      "\n",
      "Run 43:\n",
      "  Q-Agent 1 -> Delta: 0.8197, RPDI: 0.5696\n",
      "  Q-Agent 2 -> Delta: 2.1352, RPDI: 0.5590\n",
      "\n",
      "Run 44:\n",
      "  Q-Agent 1 -> Delta: 2.3517, RPDI: 0.5523\n",
      "  Q-Agent 2 -> Delta: 0.7899, RPDI: 0.5437\n",
      "\n",
      "Run 45:\n",
      "  Q-Agent 1 -> Delta: 1.1625, RPDI: 0.3773\n",
      "  Q-Agent 2 -> Delta: 0.2919, RPDI: 0.5976\n",
      "\n",
      "Run 46:\n",
      "  Q-Agent 1 -> Delta: 0.2472, RPDI: 0.4666\n",
      "  Q-Agent 2 -> Delta: 0.9861, RPDI: 0.4513\n",
      "\n",
      "Run 47:\n",
      "  Q-Agent 1 -> Delta: 0.7743, RPDI: 0.4242\n",
      "  Q-Agent 2 -> Delta: 1.4465, RPDI: 0.6877\n",
      "\n",
      "Run 48:\n",
      "  Q-Agent 1 -> Delta: 1.1790, RPDI: 0.5922\n",
      "  Q-Agent 2 -> Delta: 1.2993, RPDI: 0.4838\n",
      "\n",
      "Run 49:\n",
      "  Q-Agent 1 -> Delta: 1.3921, RPDI: 0.4529\n",
      "  Q-Agent 2 -> Delta: 0.8683, RPDI: 0.6065\n",
      "\n",
      "Run 50:\n",
      "  Q-Agent 1 -> Delta: 1.0519, RPDI: 0.6081\n",
      "  Q-Agent 2 -> Delta: 1.0862, RPDI: 0.5048\n",
      "\n",
      "================================================================================\n",
      "SUMMARY TABLE\n",
      "================================================================================\n",
      "\n",
      "    Model  Q1 Avg. Prices  Theo. Prices  Q2 Avg. Prices  Q1 Î  Q2 Î  Q1 RPDI  Q2 RPDI\n",
      "    LOGIT            1.65          1.91            1.66 -2.97 -3.10    -0.93    -0.91\n",
      "HOTELLING            1.13          1.00            1.13  0.50  0.47     0.51     0.52\n",
      "   LINEAR            0.47          0.43            0.47  1.32  1.30     0.52     0.54\n",
      "\n",
      "================================================================================\n",
      "OVERALL AVERAGES ACROSS ALL MODELS\n",
      "================================================================================\n",
      "\n",
      "Q-Agent 1: Avg Î = -0.3841, Avg RPDI = 0.0341\n",
      "Q-Agent 2: Avg Î = -0.4462, Avg RPDI = 0.0481\n",
      "\n",
      "[Results saved to ./results/q_vs_q_schemeC.csv]\n"
     ]
    }
   ],
   "source": [
    "def run_simulation(model, seed, shock_cfg, benchmarks):\n",
    "    np.random.seed(seed)\n",
    "    env = MarketEnvContinuous(market_model=model, shock_cfg=shock_cfg, seed=seed)\n",
    "    agents = [\n",
    "        QLearningAgent(env.N, agent_id=0, price_grid=env.price_grid),\n",
    "        QLearningAgent(env.N, agent_id=1, price_grid=env.price_grid)\n",
    "    ]\n",
    "    state = env.reset()\n",
    "    profits_history = []\n",
    "    prices_history = []\n",
    "    \n",
    "    for t in range(env.horizon):\n",
    "        actions = [agents[0].choose_action(state), agents[1].choose_action(state)]\n",
    "        next_state, rewards, done, info = env.step(actions)\n",
    "        agents[0].update(state, actions[0], rewards[0], next_state)\n",
    "        agents[1].update(state, actions[1], rewards[1], next_state)\n",
    "        state = next_state\n",
    "        prices_history.append(info['prices'])\n",
    "        profits_history.append(rewards)\n",
    "    \n",
    "    last_prices = np.array(prices_history[-1000:])\n",
    "    avg_price1 = np.mean(last_prices[:, 0])\n",
    "    avg_price2 = np.mean(last_prices[:, 1])\n",
    "    \n",
    "    last_profits = np.array(profits_history[-1000:])\n",
    "    avg_profit1 = np.mean(last_profits[:, 0])\n",
    "    avg_profit2 = np.mean(last_profits[:, 1])\n",
    "    \n",
    "    p_n = benchmarks['p_N']\n",
    "    p_m = benchmarks['p_M']\n",
    "    pi_n = benchmarks['E_pi_N']\n",
    "    pi_m = benchmarks['E_pi_M']\n",
    "    \n",
    "    delta1 = (avg_profit1 - pi_n) / (pi_m - pi_n) if (pi_m - pi_n) != 0 else 0\n",
    "    delta2 = (avg_profit2 - pi_n) / (pi_m - pi_n) if (pi_m - pi_n) != 0 else 0\n",
    "    \n",
    "    rpdi1 = (avg_price1 - p_n) / (p_m - p_n) if (p_m - p_n) != 0 else 0\n",
    "    rpdi2 = (avg_price2 - p_n) / (p_m - p_n) if (p_m - p_n) != 0 else 0\n",
    "    \n",
    "    return avg_price1, avg_price2, delta1, delta2, rpdi1, rpdi2, p_n\n",
    "\n",
    "\n",
    "def main():\n",
    "    shock_cfg = {\n",
    "        'enabled': True,\n",
    "        'scheme': 'C',\n",
    "        'mode': 'independent'\n",
    "    }\n",
    "    \n",
    "    benchmark_calculator = TheoreticalBenchmarks(seed=SEED)\n",
    "    all_benchmarks = benchmark_calculator.calculate_all_benchmarks(shock_cfg)\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"Q-LEARNING vs Q-LEARNING - SCHEME C\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    models = ['logit', 'hotelling', 'linear']\n",
    "    results = {}\n",
    "    run_logs = {model: {'delta1': [], 'delta2': [], 'rpdi1': [], 'rpdi2': []} for model in models}\n",
    "    \n",
    "    for model in models:\n",
    "        model_benchmarks = all_benchmarks[model]\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Model: {model.upper()}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        avg_prices1, avg_prices2 = [], []\n",
    "        deltas1, deltas2 = [], []\n",
    "        rpdis1, rpdis2 = [], []\n",
    "        \n",
    "        for run in range(NUM_RUNS):\n",
    "            seed = SEED + run\n",
    "            ap1, ap2, d1, d2, r1, r2, p_n = run_simulation(model, seed, shock_cfg, model_benchmarks)\n",
    "            avg_prices1.append(ap1)\n",
    "            avg_prices2.append(ap2)\n",
    "            deltas1.append(d1)\n",
    "            deltas2.append(d2)\n",
    "            rpdis1.append(r1)\n",
    "            rpdis2.append(r2)\n",
    "            \n",
    "            print(f\"\\nRun {run + 1}:\")\n",
    "            print(f\"  Q-Agent 1 -> Delta: {d1:.4f}, RPDI: {r1:.4f}\")\n",
    "            print(f\"  Q-Agent 2 -> Delta: {d2:.4f}, RPDI: {r2:.4f}\")\n",
    "            \n",
    "            run_logs[model]['delta1'].append(d1)\n",
    "            run_logs[model]['delta2'].append(d2)\n",
    "            run_logs[model]['rpdi1'].append(r1)\n",
    "            run_logs[model]['rpdi2'].append(r2)\n",
    "        \n",
    "        results[model] = {\n",
    "            'Avg Price 1': np.mean(avg_prices1),\n",
    "            'Avg Price 2': np.mean(avg_prices2),\n",
    "            'Theo Price': model_benchmarks['p_N'],\n",
    "            'Delta 1': np.mean(deltas1),\n",
    "            'Delta 2': np.mean(deltas2),\n",
    "            'RPDI 1': np.mean(rpdis1),\n",
    "            'RPDI 2': np.mean(rpdis2)\n",
    "        }\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"SUMMARY TABLE\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    data = {\n",
    "        'Model': [m.upper() for m in models],\n",
    "        'Q1 Avg. Prices': [round(results[m]['Avg Price 1'], 2) for m in models],\n",
    "        'Theo. Prices': [round(results[m]['Theo Price'], 2) for m in models],\n",
    "        'Q2 Avg. Prices': [round(results[m]['Avg Price 2'], 2) for m in models],\n",
    "        'Q1 Î': [round(results[m]['Delta 1'], 2) for m in models],\n",
    "        'Q2 Î': [round(results[m]['Delta 2'], 2) for m in models],\n",
    "        'Q1 RPDI': [round(results[m]['RPDI 1'], 2) for m in models],\n",
    "        'Q2 RPDI': [round(results[m]['RPDI 2'], 2) for m in models]\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    os.makedirs(\"./results\", exist_ok=True)\n",
    "    df.to_csv(\"./results/q_vs_q_schemeC.csv\", index=False)\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"OVERALL AVERAGES ACROSS ALL MODELS\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    avg_delta1 = np.mean([results[m]['Delta 1'] for m in models])\n",
    "    avg_delta2 = np.mean([results[m]['Delta 2'] for m in models])\n",
    "    avg_rpdi1 = np.mean([results[m]['RPDI 1'] for m in models])\n",
    "    avg_rpdi2 = np.mean([results[m]['RPDI 2'] for m in models])\n",
    "    \n",
    "    print(f\"Q-Agent 1: Avg Î = {avg_delta1:.4f}, Avg RPDI = {avg_rpdi1:.4f}\")\n",
    "    print(f\"Q-Agent 2: Avg Î = {avg_delta2:.4f}, Avg RPDI = {avg_rpdi2:.4f}\")\n",
    "    print(\"\\n[Results saved to ./results/q_vs_q_schemeC.csv]\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 233.80435,
   "end_time": "2025-11-26T04:02:53.273165",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-26T03:58:59.468815",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
