{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8bced2c",
   "metadata": {
    "papermill": {
     "duration": 0.0042,
     "end_time": "2025-11-25T14:09:05.395156",
     "exception": false,
     "start_time": "2025-11-25T14:09:05.390956",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Imports and Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f5f97b7",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 5.894622,
     "end_time": "2025-11-25T14:09:11.292979",
     "exception": false,
     "start_time": "2025-11-25T14:09:05.398357",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Optional, Dict, Tuple\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "from scipy.optimize import minimize_scalar\n",
    "import pandas as pd\n",
    "\n",
    "SEED = 99\n",
    "NUM_RUNS = 50\n",
    "\n",
    "os.makedirs(\"results\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c0a3bc",
   "metadata": {
    "papermill": {
     "duration": 0.003138,
     "end_time": "2025-11-25T14:09:11.299515",
     "exception": false,
     "start_time": "2025-11-25T14:09:11.296377",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# `TheoreticalBenchmarks`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b00024da",
   "metadata": {
    "papermill": {
     "duration": 0.024398,
     "end_time": "2025-11-25T14:09:11.326944",
     "exception": false,
     "start_time": "2025-11-25T14:09:11.302546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TheoreticalBenchmarks:\n",
    "    \"\"\"Calculate theoretical Nash and Monopoly prices/profits for market models\"\"\"\n",
    "    \n",
    "    def __init__(self, seed=None):\n",
    "        self.seed = seed\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "    \n",
    "    def calculate_logit_benchmarks(self, shock_cfg=None):\n",
    "        \"\"\"\n",
    "        Calculate Nash and Monopoly benchmarks for Logit model\n",
    "        With shocks: Uses Monte Carlo integration\n",
    "        Without shocks: Uses analytical formulas\n",
    "        \"\"\"\n",
    "        # Model parameters\n",
    "        a = 2.0\n",
    "        c = 1.0\n",
    "        mu = 0.25\n",
    "        a0 = 0.0\n",
    "        \n",
    "        if shock_cfg is None or not shock_cfg.get('enabled', False):\n",
    "            # No shocks - use standard benchmarks\n",
    "            p_N = 1.473\n",
    "            p_M = 1.925\n",
    "            \n",
    "            # Calculate profits at these prices\n",
    "            exp_N = np.exp((a - p_N) / mu)\n",
    "            exp_outside = np.exp(a0 / mu)\n",
    "            q_N = exp_N / (2 * exp_N + exp_outside)\n",
    "            E_pi_N = (p_N - c) * q_N\n",
    "            \n",
    "            exp_M = np.exp((a - p_M) / mu)\n",
    "            q_M = exp_M / (2 * exp_M + exp_outside)\n",
    "            E_pi_M = (p_M - c) * q_M\n",
    "            \n",
    "            return {\n",
    "                'p_N': p_N,\n",
    "                'E_pi_N': E_pi_N,\n",
    "                'p_M': p_M,\n",
    "                'E_pi_M': E_pi_M,\n",
    "                'shock_enabled': False\n",
    "            }\n",
    "        \n",
    "        # With shocks - use Monte Carlo\n",
    "        scheme_params = {\n",
    "            'A': {'rho': 0.3, 'sigma_eta': 0.5},\n",
    "            'B': {'rho': 0.95, 'sigma_eta': 0.05},\n",
    "            'C': {'rho': 0.9, 'sigma_eta': 0.3}\n",
    "        }\n",
    "        \n",
    "        scheme = shock_cfg.get('scheme', 'A')\n",
    "        params = scheme_params[scheme.upper()]\n",
    "        rho = params['rho']\n",
    "        sigma_eta = params['sigma_eta']\n",
    "        \n",
    "        # Unconditional variance of shocks\n",
    "        sigma2 = sigma_eta**2 / (1 - rho**2)\n",
    "        sigma = np.sqrt(sigma2)\n",
    "        \n",
    "        # Monte Carlo samples\n",
    "        N = 10000\n",
    "        shock_mode = shock_cfg.get('mode', 'independent')\n",
    "        \n",
    "        if shock_mode == 'independent':\n",
    "            xi_samples = np.random.normal(0, sigma, (N, 2))\n",
    "        else:  # correlated\n",
    "            xi_samples = np.random.normal(0, sigma, N)\n",
    "        \n",
    "        # Expected profit functions\n",
    "        def E_pi1(p1, p2):\n",
    "            if shock_mode == 'independent':\n",
    "                exps1 = np.exp((a - p1 + xi_samples[:, 0]) / mu)\n",
    "                exps2 = np.exp((a - p2 + xi_samples[:, 1]) / mu)\n",
    "            else:\n",
    "                exps1 = np.exp((a - p1 + xi_samples) / mu)\n",
    "                exps2 = np.exp((a - p2 + xi_samples) / mu)\n",
    "            den = exps1 + exps2 + np.exp(a0 / mu)\n",
    "            qs = exps1 / den\n",
    "            return np.mean((p1 - c) * qs)\n",
    "        \n",
    "        def E_pi2(p1, p2):\n",
    "            if shock_mode == 'independent':\n",
    "                exps1 = np.exp((a - p1 + xi_samples[:, 0]) / mu)\n",
    "                exps2 = np.exp((a - p2 + xi_samples[:, 1]) / mu)\n",
    "            else:\n",
    "                exps1 = np.exp((a - p1 + xi_samples) / mu)\n",
    "                exps2 = np.exp((a - p2 + xi_samples) / mu)\n",
    "            den = exps1 + exps2 + np.exp(a0 / mu)\n",
    "            qs = exps2 / den\n",
    "            return np.mean((p2 - c) * qs)\n",
    "        \n",
    "        # Nash equilibrium\n",
    "        def best_response(p_j):\n",
    "            def neg_E_pi(p_i):\n",
    "                return -E_pi1(p_i, p_j)\n",
    "            res = minimize_scalar(neg_E_pi, bounds=(c, c + 5), method='bounded')\n",
    "            return res.x\n",
    "        \n",
    "        p_guess = 1.5\n",
    "        for _ in range(50):\n",
    "            p_guess = best_response(p_guess)\n",
    "        p_N = p_guess\n",
    "        E_pi_N = E_pi1(p_N, p_N)\n",
    "        \n",
    "        # Monopoly\n",
    "        def neg_E_joint(p):\n",
    "            return -(E_pi1(p, p) + E_pi2(p, p))\n",
    "        \n",
    "        res_M = minimize_scalar(neg_E_joint, bounds=(c, c + 5), method='bounded')\n",
    "        p_M = res_M.x\n",
    "        E_pi_M = E_pi1(p_M, p_M)\n",
    "        \n",
    "        return {\n",
    "            'p_N': p_N,\n",
    "            'E_pi_N': E_pi_N,\n",
    "            'p_M': p_M,\n",
    "            'E_pi_M': E_pi_M,\n",
    "            'shock_enabled': True,\n",
    "            'scheme': scheme,\n",
    "            'mode': shock_mode,\n",
    "            'sigma': sigma\n",
    "        }\n",
    "    \n",
    "    def calculate_hotelling_benchmarks(self, shock_cfg=None):\n",
    "        \"\"\"\n",
    "        Calculate Nash and Monopoly benchmarks for Hotelling model\n",
    "        Shocks don't affect expected benchmarks (linearity in net shock)\n",
    "        \"\"\"\n",
    "        # Model parameters\n",
    "        c = 0.0\n",
    "        v_bar = 1.75\n",
    "        theta = 1.0\n",
    "        \n",
    "        # Standard benchmarks (unchanged with shocks)\n",
    "        p_N = 1.00\n",
    "        p_M = 1.25\n",
    "        \n",
    "        # Calculate profits\n",
    "        # At Nash: both firms charge 1, split market equally\n",
    "        q_N = 0.5\n",
    "        E_pi_N = p_N * q_N\n",
    "        \n",
    "        # At Monopoly: both charge 1.25, split market equally\n",
    "        q_M = 0.5\n",
    "        E_pi_M = p_M * q_M\n",
    "        \n",
    "        return {\n",
    "            'p_N': p_N,\n",
    "            'E_pi_N': E_pi_N,\n",
    "            'p_M': p_M,\n",
    "            'E_pi_M': E_pi_M,\n",
    "            'shock_enabled': shock_cfg is not None and shock_cfg.get('enabled', False),\n",
    "            'note': 'Shocks do not affect expected benchmarks for Hotelling'\n",
    "        }\n",
    "    \n",
    "    def calculate_linear_benchmarks(self, shock_cfg=None):\n",
    "        \"\"\"\n",
    "        Calculate Nash and Monopoly benchmarks for Linear model\n",
    "        Shocks don't affect expected benchmarks (linearity in shocks)\n",
    "        \"\"\"\n",
    "        # Model parameters\n",
    "        c = 0.0\n",
    "        a_bar = 1.0\n",
    "        d = 0.25\n",
    "        \n",
    "        # Standard benchmarks (unchanged with shocks)\n",
    "        p_N = 0.4286\n",
    "        p_M = 0.5\n",
    "        \n",
    "        # Calculate profits\n",
    "        denominator = 1 - d**2\n",
    "        \n",
    "        # At Nash\n",
    "        q_N = (a_bar - p_N - d * (a_bar - p_N)) / denominator\n",
    "        E_pi_N = p_N * q_N\n",
    "        \n",
    "        # At Monopoly\n",
    "        q_M = a_bar - p_M\n",
    "        E_pi_M = p_M * q_M\n",
    "        \n",
    "        return {\n",
    "            'p_N': p_N,\n",
    "            'E_pi_N': E_pi_N,\n",
    "            'p_M': p_M,\n",
    "            'E_pi_M': E_pi_M,\n",
    "            'shock_enabled': shock_cfg is not None and shock_cfg.get('enabled', False),\n",
    "            'note': 'Shocks do not affect expected benchmarks for Linear'\n",
    "        }\n",
    "    \n",
    "    def calculate_all_benchmarks(self, shock_cfg=None):\n",
    "        \"\"\"Calculate benchmarks for all three models\"\"\"\n",
    "        results = {\n",
    "            'logit': self.calculate_logit_benchmarks(shock_cfg),\n",
    "            'hotelling': self.calculate_hotelling_benchmarks(shock_cfg),\n",
    "            'linear': self.calculate_linear_benchmarks(shock_cfg)\n",
    "        }\n",
    "        return results\n",
    "    \n",
    "    def generate_benchmark_table(self, shock_configs):\n",
    "        \"\"\"\n",
    "        Generate a comprehensive table of benchmarks for different configurations\n",
    "        \n",
    "        Args:\n",
    "            shock_configs: List of shock configurations to test\n",
    "        \n",
    "        Returns:\n",
    "            pandas DataFrame with benchmarks\n",
    "        \"\"\"\n",
    "        data = []\n",
    "        \n",
    "        for config in shock_configs:\n",
    "            config_name = config.get('name', 'No Config')\n",
    "            benchmarks = self.calculate_all_benchmarks(config)\n",
    "            \n",
    "            for model, bench in benchmarks.items():\n",
    "                data.append({\n",
    "                    'Configuration': config_name,\n",
    "                    'Model': model.upper(),\n",
    "                    'Nash Price': round(bench['p_N'], 4),\n",
    "                    'Nash Profit': round(bench['E_pi_N'], 4),\n",
    "                    'Monopoly Price': round(bench['p_M'], 4),\n",
    "                    'Monopoly Profit': round(bench['E_pi_M'], 4),\n",
    "                    'Shock Enabled': bench['shock_enabled']\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f226756e",
   "metadata": {
    "papermill": {
     "duration": 0.002962,
     "end_time": "2025-11-25T14:09:11.333092",
     "exception": false,
     "start_time": "2025-11-25T14:09:11.330130",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# `MarketEnv` Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dccece6f",
   "metadata": {
    "papermill": {
     "duration": 0.030987,
     "end_time": "2025-11-25T14:09:11.367045",
     "exception": false,
     "start_time": "2025-11-25T14:09:11.336058",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MarketEnv:\n",
    "    \"\"\"Base Market environment supporting all three models with optional shocks\"\"\"\n",
    "   \n",
    "    def __init__(\n",
    "        self,\n",
    "        market_model: str = \"logit\",\n",
    "        shock_cfg: Optional[Dict] = None,\n",
    "        n_firms: int = 2,\n",
    "        horizon: int = 10000,\n",
    "        seed: Optional[int] = None\n",
    "    ):\n",
    "        self.model = market_model.lower()\n",
    "        self.n_firms = n_firms\n",
    "        self.N = 15 # Number of discrete prices\n",
    "        self.horizon = horizon\n",
    "        self.t = 0\n",
    "        self.rng = np.random.RandomState(seed)\n",
    "       \n",
    "        # Model-specific parameters (verified against PDFs)\n",
    "        if self.model == \"logit\":\n",
    "            self.c = 1.0\n",
    "            self.a = 2.0\n",
    "            self.a0 = 0.0\n",
    "            self.mu = 0.25\n",
    "            self.P_N = 1.473\n",
    "            self.P_M = 1.925\n",
    "        elif self.model == \"hotelling\":\n",
    "            self.c = 0.0\n",
    "            self.v_bar = 1.75\n",
    "            self.theta = 1.0\n",
    "            self.P_N = 1.00\n",
    "            self.P_M = 1.25\n",
    "        elif self.model == \"linear\":\n",
    "            self.c = 0.0\n",
    "            self.a_bar = 1.0\n",
    "            self.d = 0.25\n",
    "            \n",
    "            # Nash (Cournot duopoly)\n",
    "            self.P_N = self.a_bar * (1 - self.d) / (2 - self.d)  # 0.4286\n",
    "            q_N = self.P_N  # By symmetry at equilibrium\n",
    "            pi_N = self.P_N * q_N  # 0.1959\n",
    "            \n",
    "            # Monopoly (single firm, no competition)\n",
    "            self.P_M = self.a_bar / 2  # 0.5\n",
    "            q_M = self.a_bar - self.P_M  # 0.5 (residual demand, NOT Cournot!)\n",
    "            pi_M = self.P_M * q_M  # 0.25\n",
    "            \n",
    "            # Store for Delta calculations\n",
    "            self.pi_N_linear = pi_N  # 0.1959\n",
    "            self.pi_M_linear = pi_M  # 0.25\n",
    "            \n",
    "            # Verify profit range is healthy\n",
    "            profit_range = pi_M - pi_N  # Should be ~0.05\n",
    "            if profit_range < 0.04:\n",
    "                import warnings\n",
    "                warnings.warn(f\"Linear model profit range too small: {profit_range:.4f}\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model: {self.model}\")\n",
    "       \n",
    "        # Construct price grid\n",
    "        span = self.P_M - self.P_N\n",
    "        self.price_grid = np.linspace(\n",
    "            self.P_N - 0.15 * span,\n",
    "            self.P_M + 0.15 * span,\n",
    "            self.N\n",
    "        )\n",
    "       \n",
    "        # Initialize shock configuration\n",
    "        self.shock_cfg = shock_cfg or {}\n",
    "        self.shock_enabled = self.shock_cfg.get(\"enabled\", False)\n",
    "       \n",
    "        if self.shock_enabled:\n",
    "            self.shock_mode = self.shock_cfg.get(\"mode\", \"correlated\")\n",
    "            scheme = self.shock_cfg.get(\"scheme\", None)\n",
    "           \n",
    "            # Get AR(1) parameters (verified against PDFs)\n",
    "            if scheme:\n",
    "                scheme_params = {\n",
    "                    'A': {'rho': 0.3, 'sigma_eta': 0.5},\n",
    "                    'B': {'rho': 0.95, 'sigma_eta': 0.05},\n",
    "                    'C': {'rho': 0.9, 'sigma_eta': 0.3}\n",
    "                }\n",
    "                if scheme.upper() in scheme_params:\n",
    "                    params = scheme_params[scheme.upper()]\n",
    "                    self.rho = params['rho']\n",
    "                    self.sigma_eta = params['sigma_eta']\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown scheme: {scheme}\")\n",
    "            else:\n",
    "                self.rho = self.shock_cfg.get('rho', 0.9)\n",
    "                self.sigma_eta = self.shock_cfg.get('sigma_eta', 0.15)\n",
    "           \n",
    "            # Initialize shock generators\n",
    "            if self.shock_mode == \"independent\":\n",
    "                self.shock_generators = [\n",
    "                    AR1_Shock(self.rho, self.sigma_eta, seed=seed+i if seed else None)\n",
    "                    for i in range(self.n_firms)\n",
    "                ]\n",
    "            else: # correlated\n",
    "                self.shock_generator = AR1_Shock(self.rho, self.sigma_eta, seed=seed)\n",
    "           \n",
    "            self.current_shocks = np.zeros(self.n_firms)\n",
    "       \n",
    "        self.reset()\n",
    "   \n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment - returns state as actual prices\"\"\"\n",
    "        self.t = 0\n",
    "       \n",
    "        # Reset shocks\n",
    "        if self.shock_enabled:\n",
    "            if self.shock_mode == \"independent\":\n",
    "                for gen in self.shock_generators:\n",
    "                    gen.reset()\n",
    "            else:\n",
    "                self.shock_generator.reset()\n",
    "            self.current_shocks = np.zeros(self.n_firms)\n",
    "       \n",
    "        # Initialize prices at middle of grid\n",
    "        mid_idx = self.N // 2\n",
    "        self.current_price_idx = np.array([mid_idx] * self.n_firms)\n",
    "       \n",
    "        return self._get_state()\n",
    "   \n",
    "    def _get_state(self):\n",
    "        \"\"\"Get current state as actual prices (numpy array)\"\"\"\n",
    "        return self.price_grid[self.current_price_idx].copy()\n",
    "    \n",
    "    def _get_state_indices(self):\n",
    "        \"\"\"Get current state as price indices (for Q-learning compatibility)\"\"\"\n",
    "        return tuple(self.current_price_idx)\n",
    "   \n",
    "    def _generate_shocks(self):\n",
    "        \"\"\"Generate next period shocks\"\"\"\n",
    "        if not self.shock_enabled:\n",
    "            return np.zeros(self.n_firms)\n",
    "       \n",
    "        if self.shock_mode == \"independent\":\n",
    "            shocks = np.array([gen.generate_next() for gen in self.shock_generators])\n",
    "        else:\n",
    "            shock = self.shock_generator.generate_next()\n",
    "            shocks = np.array([shock] * self.n_firms)\n",
    "       \n",
    "        return shocks\n",
    "   \n",
    "    def get_current_shocks(self):\n",
    "        \"\"\"Get current shocks (for PSO evaluation)\"\"\"\n",
    "        return self.current_shocks.copy() if self.shock_enabled else np.zeros(self.n_firms)\n",
    "   \n",
    "    def calculate_demand_and_profit(self, prices: np.ndarray, shocks: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Calculate demand and profit given prices and shocks\"\"\"\n",
    "        if self.model == \"logit\":\n",
    "            return self._logit_demand_profit(prices, shocks)\n",
    "        elif self.model == \"hotelling\":\n",
    "            return self._hotelling_demand_profit(prices, shocks)\n",
    "        elif self.model == \"linear\":\n",
    "            return self._linear_demand_profit(prices, shocks)\n",
    "   \n",
    "    def _logit_demand_profit(self, prices: np.ndarray, shocks: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Logit demand - shocks affect variance but not Nash equilibrium\n",
    "        \n",
    "        Critical: E[demand | shocks] = demand(no shocks) since E[ε] = 0\n",
    "        Nash price must remain constant regardless of shock scheme\n",
    "        \"\"\"\n",
    "        # Base utilities WITHOUT shocks (defines Nash equilibrium)\n",
    "        base_utilities = (self.a - prices) / self.mu\n",
    "        \n",
    "        # Add shocks OUTSIDE mu scaling to preserve E[utility]\n",
    "        realized_utilities = base_utilities + shocks\n",
    "        \n",
    "        # Numerical stability\n",
    "        max_util = np.max(realized_utilities)\n",
    "        exp_utils = np.exp(realized_utilities - max_util)\n",
    "        exp_outside = np.exp(self.a0/self.mu - max_util)\n",
    "        \n",
    "        denominator = np.sum(exp_utils) + exp_outside\n",
    "        demands = exp_utils / denominator\n",
    "        profits = (prices - self.c) * demands\n",
    "        \n",
    "        return demands, profits\n",
    "   \n",
    "    def _hotelling_demand_profit(self, prices: np.ndarray, shocks: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Hotelling demand with net shock affecting boundary (verified against PDF)\"\"\"\n",
    "        p1, p2 = prices[0], prices[1]\n",
    "        epsilon_net = shocks[0] - shocks[1] if self.n_firms == 2 else 0\n",
    "       \n",
    "        x_hat = 0.5 + (p2 - p1 + epsilon_net) / (2 * self.theta)\n",
    "        x_hat = np.clip(x_hat, 0, 1)\n",
    "       \n",
    "        q1 = x_hat\n",
    "        q2 = 1 - x_hat\n",
    "        demands = np.array([q1, q2])\n",
    "        profits = prices * demands\n",
    "       \n",
    "        return demands, profits\n",
    "   \n",
    "    def _linear_demand_profit(self, prices: np.ndarray, shocks: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Linear demand with additive shocks (verified against PDF)\"\"\"\n",
    "        a_shocked = self.a_bar + shocks\n",
    "        denominator = 1 - self.d**2\n",
    "       \n",
    "        q1 = ((a_shocked[0] - prices[0]) - self.d * (a_shocked[1] - prices[1])) / denominator\n",
    "        q2 = ((a_shocked[1] - prices[1]) - self.d * (a_shocked[0] - prices[0])) / denominator\n",
    "       \n",
    "        q1 = max(0, q1)\n",
    "        q2 = max(0, q2)\n",
    "        demands = np.array([q1, q2])\n",
    "        profits = prices * demands\n",
    "       \n",
    "        return demands, profits\n",
    "   \n",
    "    def step(self, action_indices):\n",
    "        \"\"\"Execute one step - returns (state_prices, profits, done, info)\n",
    "        \n",
    "        Args:\n",
    "            action_indices: Array of price indices for each agent\n",
    "            \n",
    "        Returns:\n",
    "            next_state: Numpy array of actual prices (not indices)\n",
    "            profits: Numpy array of profits for each agent\n",
    "            done: Episode termination flag\n",
    "            info: Dictionary with prices, demands, and shocks\n",
    "        \"\"\"\n",
    "        action_indices = np.asarray(action_indices, dtype=int)\n",
    "       \n",
    "        # Update price indices\n",
    "        self.current_price_idx = action_indices\n",
    "        prices = self.price_grid[self.current_price_idx]\n",
    "       \n",
    "        # Generate shocks for this period\n",
    "        self.current_shocks = self._generate_shocks()\n",
    "       \n",
    "        # Calculate demands and profits\n",
    "        demands, profits = self.calculate_demand_and_profit(prices, self.current_shocks)\n",
    "       \n",
    "        # Update time\n",
    "        self.t += 1\n",
    "       \n",
    "        # Get next state (actual prices, not indices)\n",
    "        next_state = self._get_state()\n",
    "       \n",
    "        # Episode termination\n",
    "        done = False\n",
    "       \n",
    "        # Info dictionary\n",
    "        info = {\n",
    "            'prices': prices.copy(),\n",
    "            'demands': demands.copy(),\n",
    "            'shocks': self.current_shocks.copy(),\n",
    "            'price_indices': self.current_price_idx.copy()  # Include indices for Q-learning\n",
    "        }\n",
    "       \n",
    "        return next_state, profits, done, info\n",
    "\n",
    "class MarketEnvContinuous(MarketEnv):\n",
    "    \"\"\"Market environment that accepts both discrete indices and continuous prices as actions\"\"\"\n",
    "    \n",
    "    def step(self, actions):\n",
    "        \"\"\"Execute one step with flexible action handling\n",
    "        \n",
    "        Args:\n",
    "            actions: List/array where each element can be:\n",
    "                     - int/np.integer: discrete price index\n",
    "                     - float: continuous price value\n",
    "                     \n",
    "        Returns:\n",
    "            next_state: Numpy array of actual prices\n",
    "            profits: Numpy array of profits\n",
    "            done: Episode termination flag\n",
    "            info: Dictionary with prices, demands, shocks, and indices\n",
    "        \"\"\"\n",
    "        prices = []\n",
    "        indices = []\n",
    "        \n",
    "        for a in actions:\n",
    "            if isinstance(a, (int, np.integer)):\n",
    "                # Discrete action: use price from grid\n",
    "                prices.append(self.price_grid[a])\n",
    "                indices.append(a)\n",
    "            else:\n",
    "                # Continuous action: use actual price value\n",
    "                price = float(a)\n",
    "                prices.append(price)\n",
    "                # Find closest grid index for tracking\n",
    "                indices.append(np.argmin(np.abs(self.price_grid - price)))\n",
    "        \n",
    "        prices = np.array(prices)\n",
    "        self.current_price_idx = np.array(indices)\n",
    "        \n",
    "        # Generate shocks\n",
    "        self.current_shocks = self._generate_shocks()\n",
    "        \n",
    "        # Calculate demands and profits\n",
    "        demands, profits = self.calculate_demand_and_profit(prices, self.current_shocks)\n",
    "        \n",
    "        # Update time\n",
    "        self.t += 1\n",
    "        \n",
    "        # Get next state (actual prices)\n",
    "        next_state = self._get_state()\n",
    "        \n",
    "        # Episode termination\n",
    "        done = False\n",
    "        \n",
    "        # Info dictionary\n",
    "        info = {\n",
    "            'prices': prices.copy(),\n",
    "            'demands': demands.copy(),\n",
    "            'shocks': self.current_shocks.copy(),\n",
    "            'price_indices': self.current_price_idx.copy()\n",
    "        }\n",
    "        \n",
    "        return next_state, profits, done, info\n",
    "\n",
    "    def get_state_prices(self):\n",
    "        \"\"\"Get current state as actual prices (same as _get_state)\"\"\"\n",
    "        return self._get_state()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66917628",
   "metadata": {
    "papermill": {
     "duration": 0.003089,
     "end_time": "2025-11-25T14:09:11.373278",
     "exception": false,
     "start_time": "2025-11-25T14:09:11.370189",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# `DQNAgent`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00e5650a",
   "metadata": {
    "papermill": {
     "duration": 0.021798,
     "end_time": "2025-11-25T14:09:11.398134",
     "exception": false,
     "start_time": "2025-11-25T14:09:11.376336",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \"\"\"\n",
    "    Deep Q-Network Agent implementation following Mnih et al. (2015)\n",
    "    and the dynamic pricing paper specifications.\n",
    "   \n",
    "    Key features:\n",
    "    - Uses continuous state representation (actual prices, not indices)\n",
    "    - Implements Double DQN with separate target network\n",
    "    - Proper experience replay with adequate buffer size\n",
    "    - Gradient clipping and Huber loss for stability\n",
    "    - GPU acceleration for faster training in pricing competitions\n",
    "    \"\"\"\n",
    "   \n",
    "    def __init__(self, agent_id, state_dim=2, action_dim=15, seed=None):\n",
    "        \"\"\"\n",
    "        Initialize DQN Agent.\n",
    "       \n",
    "        Args:\n",
    "            agent_id: Agent identifier (0 or 1)\n",
    "            state_dim: Dimension of state space (2 for price tuple)\n",
    "            action_dim: Number of discrete actions (price grid size)\n",
    "            seed: Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.agent_id = agent_id\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "       \n",
    "        # Hyperparameters from papers\n",
    "        self.gamma = 0.99 # Discount factor\n",
    "        self.epsilon = 1.0 # Initial exploration rate\n",
    "        self.epsilon_min = 0.01 # Minimum exploration rate\n",
    "        self.epsilon_decay = 0.995 # Decay rate per episode\n",
    "        self.learning_rate = 0.0001 # Adam optimizer learning rate\n",
    "        self.batch_size = 128 # Minibatch size for training\n",
    "        self.memory_size = 50000 # Experience replay buffer size\n",
    "        self.target_update_freq = 500 # Steps between target network updates\n",
    "       \n",
    "        # Set random seeds if provided\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "            if self.device.type == 'cuda':\n",
    "                torch.cuda.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            random.seed(seed)\n",
    "       \n",
    "        # For deterministic behavior on GPU\n",
    "        if self.device.type == 'cuda':\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "       \n",
    "        # Initialize neural networks\n",
    "        self.network = self._build_network().to(self.device)\n",
    "        self.target_network = self._build_network().to(self.device)\n",
    "       \n",
    "        # Copy weights to target network\n",
    "        self.target_network.load_state_dict(self.network.state_dict())\n",
    "        self.target_network.eval() # Set target network to evaluation mode\n",
    "       \n",
    "        # Initialize optimizer\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=self.learning_rate)\n",
    "       \n",
    "        # Initialize experience replay buffer\n",
    "        self.memory = deque(maxlen=self.memory_size)\n",
    "       \n",
    "        # Step counter for target network updates\n",
    "        self.steps = 0\n",
    "       \n",
    "    def _build_network(self):\n",
    "        \"\"\"\n",
    "        Build the neural network architecture.\n",
    "        Deeper network than original for better representation learning.\n",
    "        \"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(self.state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, self.action_dim)\n",
    "        )\n",
    "   \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Store experience in replay buffer.\n",
    "       \n",
    "        Args:\n",
    "            state: Current state (np.array of prices)\n",
    "            action: Action taken (index)\n",
    "            reward: Reward received\n",
    "            next_state: Next state (np.array of prices)\n",
    "            done: Episode termination flag\n",
    "        \"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "   \n",
    "    def select_action(self, state, explore=True):\n",
    "        \"\"\"\n",
    "        Select action using epsilon-greedy policy.\n",
    "       \n",
    "        Args:\n",
    "            state: Current state as np.array([own_price, opponent_price])\n",
    "            explore: Whether to use exploration (training) or exploitation (evaluation)\n",
    "       \n",
    "        Returns:\n",
    "            action: Selected action index\n",
    "        \"\"\"\n",
    "        # Epsilon-greedy exploration\n",
    "        if explore and np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.action_dim)\n",
    "       \n",
    "        # Exploitation: choose best action according to Q-network\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            q_values = self.network(state_tensor)\n",
    "            return q_values.argmax().item()\n",
    "   \n",
    "    def replay(self):\n",
    "        \"\"\"\n",
    "        Train the network on a minibatch sampled from experience replay.\n",
    "        Implements Double DQN with target network.\n",
    "        \"\"\"\n",
    "        # Need minimum experiences before training\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "       \n",
    "        # Sample minibatch from replay buffer\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "       \n",
    "        # Separate batch components\n",
    "        states = torch.FloatTensor([e[0] for e in batch]).to(self.device)\n",
    "        actions = torch.LongTensor([e[1] for e in batch]).to(self.device)\n",
    "        rewards = torch.FloatTensor([e[2] for e in batch]).to(self.device)\n",
    "        next_states = torch.FloatTensor([e[3] for e in batch]).to(self.device)\n",
    "        dones = torch.FloatTensor([e[4] for e in batch]).to(self.device)\n",
    "       \n",
    "        # Current Q values for taken actions\n",
    "        current_q_values = self.network(states).gather(1, actions.unsqueeze(1))\n",
    "       \n",
    "        # Double DQN implementation\n",
    "        # Step 1: Use main network to select best actions for next states\n",
    "        with torch.no_grad():\n",
    "            next_actions = self.network(next_states).argmax(1)\n",
    "           \n",
    "        # Step 2: Use target network to evaluate those actions\n",
    "        next_q_values = self.target_network(next_states).gather(\n",
    "            1, next_actions.unsqueeze(1)\n",
    "        ).squeeze(1)\n",
    "       \n",
    "        # Compute targets (Bellman equation)\n",
    "        targets = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "       \n",
    "        # Compute loss (Huber loss for stability)\n",
    "        loss = F.smooth_l1_loss(current_q_values.squeeze(), targets.detach())\n",
    "       \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "       \n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(self.network.parameters(), max_norm=1.0)\n",
    "       \n",
    "        self.optimizer.step()\n",
    "       \n",
    "        # Increment step counter\n",
    "        self.steps += 1\n",
    "       \n",
    "        # Periodically update target network\n",
    "        if self.steps % self.target_update_freq == 0:\n",
    "            self.update_target_network()\n",
    "   \n",
    "    def update_target_network(self):\n",
    "        \"\"\"\n",
    "        Copy weights from main network to target network.\n",
    "        This stabilizes training by keeping targets fixed for multiple updates.\n",
    "        \"\"\"\n",
    "        self.target_network.load_state_dict(self.network.state_dict())\n",
    "   \n",
    "    def update_epsilon(self):\n",
    "        \"\"\"\n",
    "        Decay exploration rate.\n",
    "        Called at the end of each episode or at regular intervals.\n",
    "        \"\"\"\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "   \n",
    "    def save(self, filepath):\n",
    "        \"\"\"\n",
    "        Save model weights.\n",
    "       \n",
    "        Args:\n",
    "            filepath: Path to save the model\n",
    "        \"\"\"\n",
    "        torch.save({\n",
    "            'network_state_dict': self.network.state_dict(),\n",
    "            'target_network_state_dict': self.target_network.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'epsilon': self.epsilon,\n",
    "            'steps': self.steps\n",
    "        }, filepath)\n",
    "   \n",
    "    def load(self, filepath):\n",
    "        \"\"\"\n",
    "        Load model weights.\n",
    "       \n",
    "        Args:\n",
    "            filepath: Path to load the model from\n",
    "        \"\"\"\n",
    "        checkpoint = torch.load(filepath, map_location=self.device)\n",
    "        self.network.load_state_dict(checkpoint['network_state_dict'])\n",
    "        self.target_network.load_state_dict(checkpoint['target_network_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.epsilon = checkpoint['epsilon']\n",
    "        self.steps = checkpoint['steps']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ce57e3",
   "metadata": {
    "papermill": {
     "duration": 0.003138,
     "end_time": "2025-11-25T14:09:11.404331",
     "exception": false,
     "start_time": "2025-11-25T14:09:11.401193",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# `DDPGAgent`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edb0b5fa",
   "metadata": {
    "papermill": {
     "duration": 0.029232,
     "end_time": "2025-11-25T14:09:11.436577",
     "exception": false,
     "start_time": "2025-11-25T14:09:11.407345",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Experience replay buffer for DDPG\"\"\"\n",
    "    def __init__(self, capacity=100000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "   \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "   \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (np.array(states), np.array(actions), np.array(rewards),\n",
    "                np.array(next_states), np.array(dones))\n",
    "   \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor network for DDPG\"\"\"\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=400):\n",
    "        super(Actor, self).__init__()\n",
    "        self.bn_input = nn.BatchNorm1d(state_dim)\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 300)\n",
    "        self.bn2 = nn.BatchNorm1d(300)\n",
    "        self.fc3 = nn.Linear(300, action_dim)\n",
    "       \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "   \n",
    "    def _init_weights(self):\n",
    "        nn.init.uniform_(self.fc1.weight, -1/np.sqrt(self.fc1.in_features), 1/np.sqrt(self.fc1.in_features))\n",
    "        nn.init.uniform_(self.fc2.weight, -1/np.sqrt(self.fc2.in_features), 1/np.sqrt(self.fc2.in_features))\n",
    "        nn.init.uniform_(self.fc3.weight, -3e-3, 3e-3)\n",
    "   \n",
    "    def forward(self, state):\n",
    "        x = self.bn_input(state)\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic network for DDPG\"\"\"\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=400):\n",
    "        super(Critic, self).__init__()\n",
    "        self.bn_input = nn.BatchNorm1d(state_dim)\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim + action_dim, 300)\n",
    "        self.fc3 = nn.Linear(300, 1)\n",
    "       \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "   \n",
    "    def _init_weights(self):\n",
    "        nn.init.uniform_(self.fc1.weight, -1/np.sqrt(self.fc1.in_features), 1/np.sqrt(self.fc1.in_features))\n",
    "        nn.init.uniform_(self.fc2.weight, -1/np.sqrt(self.fc2.in_features), 1/np.sqrt(self.fc2.in_features))\n",
    "        nn.init.uniform_(self.fc3.weight, -3e-3, 3e-3)\n",
    "   \n",
    "    def forward(self, state, action):\n",
    "        x = self.bn_input(state)\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = torch.cat([x, action], dim=1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process for exploration\"\"\"\n",
    "    def __init__(self, action_dim, mu=0.0, theta=0.15, sigma=0.2):\n",
    "        self.action_dim = action_dim\n",
    "        self.mu = mu\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.state = np.ones(self.action_dim) * self.mu\n",
    "        self.reset()\n",
    "   \n",
    "    def reset(self):\n",
    "        self.state = np.ones(self.action_dim) * self.mu\n",
    "   \n",
    "    def sample(self):\n",
    "        dx = self.theta * (self.mu - self.state)\n",
    "        dx += self.sigma * np.random.randn(self.action_dim)\n",
    "        self.state += dx\n",
    "        return self.state.copy()\n",
    "\n",
    "class DDPGAgent:\n",
    "    \"\"\"Deep Deterministic Policy Gradient Agent for pricing competition simulation\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        agent_id,\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        hidden_dim=400,\n",
    "        actor_lr=1e-4,\n",
    "        critic_lr=1e-3,\n",
    "        gamma=0.99,\n",
    "        tau=0.001,\n",
    "        buffer_size=1000000,\n",
    "        batch_size=64,\n",
    "        seed=None,\n",
    "        price_min=0.0,\n",
    "        price_max=2.0\n",
    "    ):\n",
    "        self.agent_id = agent_id\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "        self.price_min = price_min\n",
    "        self.price_max = price_max\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "            if self.device.type == 'cuda':\n",
    "                torch.cuda.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            random.seed(seed)\n",
    "        \n",
    "        # For deterministic behavior on GPU\n",
    "        if self.device.type == 'cuda':\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "        \n",
    "        # Actor networks\n",
    "        self.actor = Actor(state_dim, action_dim, hidden_dim).to(self.device)\n",
    "        self.actor_target = Actor(state_dim, action_dim, hidden_dim).to(self.device)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        \n",
    "        # Critic networks\n",
    "        self.critic = Critic(state_dim, action_dim, hidden_dim).to(self.device)\n",
    "        self.critic_target = Critic(state_dim, action_dim, hidden_dim).to(self.device)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr, weight_decay=1e-2)\n",
    "        \n",
    "        # Replay buffer\n",
    "        self.replay_buffer = ReplayBuffer(buffer_size)\n",
    "        \n",
    "        # Noise process\n",
    "        self.noise = OUNoise(action_dim)\n",
    "        \n",
    "        # Exploration parameters\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "   \n",
    "    def select_action(self, state, explore=True):\n",
    "        \"\"\"Select action using actor network with optional exploration noise\"\"\"\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        self.actor.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor(state).cpu().numpy()[0]\n",
    "        self.actor.train()\n",
    "        \n",
    "        if explore:\n",
    "            noise = self.noise.sample() * self.epsilon\n",
    "            action += noise\n",
    "            action = np.clip(action, -1, 1)\n",
    "        \n",
    "        normalized_action = action.copy()  # For remember/replay (normalized [-1,1])\n",
    "        \n",
    "        # Scale to [price_min, price_max]\n",
    "        scaled_price = self.price_min + (self.price_max - self.price_min) * (action[0] + 1) / 2\n",
    "        \n",
    "        return scaled_price, normalized_action\n",
    "   \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store experience in replay buffer\"\"\"\n",
    "        self.replay_buffer.push(state, action, reward, next_state, done)\n",
    "   \n",
    "    def replay(self):\n",
    "        \"\"\"Train on a batch of experiences from replay buffer\"\"\"\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "       \n",
    "        # Sample batch\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "       \n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.FloatTensor(actions).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1).to(self.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).unsqueeze(1).to(self.device)\n",
    "       \n",
    "        # Update critic\n",
    "        with torch.no_grad():\n",
    "            next_actions = self.actor_target(next_states)\n",
    "            target_q = self.critic_target(next_states, next_actions)\n",
    "            target_q = rewards + (1 - dones) * self.gamma * target_q\n",
    "       \n",
    "        current_q = self.critic(states, actions)\n",
    "        critic_loss = F.mse_loss(current_q, target_q)\n",
    "       \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "       \n",
    "        # Update actor\n",
    "        actor_loss = -self.critic(states, self.actor(states)).mean()\n",
    "       \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "       \n",
    "        # Soft update target networks\n",
    "        self._soft_update(self.actor, self.actor_target)\n",
    "        self._soft_update(self.critic, self.critic_target)\n",
    "   \n",
    "    def _soft_update(self, local_model, target_model):\n",
    "        \"\"\"Soft update target network parameters: θ_target = τ*θ_local + (1 - τ)*θ_target\"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(self.tau * local_param.data + (1.0 - self.tau) * target_param.data)\n",
    "   \n",
    "    def update_epsilon(self):\n",
    "        \"\"\"Decay exploration rate\"\"\"\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "   \n",
    "    def reset_noise(self):\n",
    "        \"\"\"Reset the noise process\"\"\"\n",
    "        self.noise.reset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6685e4d8",
   "metadata": {
    "papermill": {
     "duration": 0.002699,
     "end_time": "2025-11-25T14:09:11.442215",
     "exception": false,
     "start_time": "2025-11-25T14:09:11.439516",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a3a987d",
   "metadata": {
    "papermill": {
     "duration": 3539.803139,
     "end_time": "2025-11-25T15:08:11.248187",
     "exception": false,
     "start_time": "2025-11-25T14:09:11.445048",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DQN vs DDPG - SCHEME NONE\n",
      "================================================================================\n",
      "\n",
      "Running LOGIT simulations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_202024/3347567576.py:131: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n",
      "  states = torch.FloatTensor([e[0] for e in batch]).to(self.device)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 175\u001b[39m\n\u001b[32m    171\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[Results saved to ./results/dqn_vs_ddpg.csv]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 113\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m run \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_RUNS):\n\u001b[32m    112\u001b[39m     seed = SEED + run\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     ap_dqn, ap_ddpg, d_dqn, d_ddpg, r_dqn, r_ddpg, p_n = \u001b[43mrun_simulation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshock_cfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_benchmarks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    114\u001b[39m     avg_prices_dqn.append(ap_dqn)\n\u001b[32m    115\u001b[39m     avg_prices_ddpg.append(ap_ddpg)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mrun_simulation\u001b[39m\u001b[34m(model, seed, shock_cfg, benchmarks)\u001b[39m\n\u001b[32m     43\u001b[39m next_ddpg_state = next_state.astype(np.float32)\n\u001b[32m     44\u001b[39m ddpg_agent.remember(ddpg_state, ddpg_norm, rewards[\u001b[32m1\u001b[39m], next_ddpg_state, done)\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[43mddpg_agent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Decay exploration\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m t % \u001b[32m100\u001b[39m == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 202\u001b[39m, in \u001b[36mDDPGAgent.replay\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;28mself\u001b[39m.critic_optimizer.zero_grad()\n\u001b[32m    201\u001b[39m critic_loss.backward()\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcritic_optimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;66;03m# Update actor\u001b[39;00m\n\u001b[32m    205\u001b[39m actor_loss = -\u001b[38;5;28mself\u001b[39m.critic(states, \u001b[38;5;28mself\u001b[39m.actor(states)).mean()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Pricing-Competition/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:517\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    512\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    513\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    514\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    515\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m517\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    520\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Pricing-Competition/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:82\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     80\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     81\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     84\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Pricing-Competition/.venv/lib/python3.12/site-packages/torch/optim/adam.py:247\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    235\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    237\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    238\u001b[39m         group,\n\u001b[32m    239\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    244\u001b[39m         state_steps,\n\u001b[32m    245\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoupled_weight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Pricing-Competition/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:150\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Pricing-Competition/.venv/lib/python3.12/site-packages/torch/optim/adam.py:953\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    950\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    951\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m953\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Pricing-Competition/.venv/lib/python3.12/site-packages/torch/optim/adam.py:466\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    462\u001b[39m         exp_avg_sq.mul_(beta2).addcmul_(\n\u001b[32m    463\u001b[39m             grad, grad, value=cast(\u001b[38;5;28mfloat\u001b[39m, \u001b[32m1\u001b[39m - beta2)\n\u001b[32m    464\u001b[39m         )\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m466\u001b[39m     \u001b[43mexp_avg_sq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m.addcmul_(grad, grad, value=\u001b[32m1\u001b[39m - beta2)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    468\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[32m    469\u001b[39m     step = step_t\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def run_simulation(model, seed, shock_cfg, benchmarks):\n",
    "    \"\"\"Run DQN vs DDPG simulation\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    env = MarketEnvContinuous(market_model=model, shock_cfg=shock_cfg, seed=seed)\n",
    "    \n",
    "    price_min = env.price_grid.min()\n",
    "    price_max = env.price_grid.max()\n",
    "    \n",
    "    # Initialize DQN agent (agent 0)\n",
    "    dqn_agent = DQNAgent(agent_id=0, state_dim=2, action_dim=env.N, seed=seed)\n",
    "    \n",
    "    # Initialize DDPG agent (agent 1)\n",
    "    ddpg_agent = DDPGAgent(\n",
    "        agent_id=1,\n",
    "        state_dim=2,\n",
    "        action_dim=1,\n",
    "        seed=seed,\n",
    "        price_min=price_min,\n",
    "        price_max=price_max\n",
    "    )\n",
    "    \n",
    "    state = env.reset()\n",
    "    profits_history = []\n",
    "    prices_history = []\n",
    "    \n",
    "    for t in range(env.horizon):\n",
    "        # DQN selects discrete action index\n",
    "        dqn_action = dqn_agent.select_action(state, explore=True)\n",
    "        \n",
    "        # DDPG selects continuous price\n",
    "        ddpg_state = state.astype(np.float32)\n",
    "        ddpg_price, ddpg_norm = ddpg_agent.select_action(ddpg_state, explore=True)\n",
    "        \n",
    "        actions = [dqn_action, ddpg_price]\n",
    "        next_state, rewards, done, info = env.step(actions)\n",
    "        \n",
    "        # Update DQN\n",
    "        dqn_agent.remember(state, dqn_action, rewards[0], next_state, done)\n",
    "        dqn_agent.replay()\n",
    "        \n",
    "        # Update DDPG\n",
    "        next_ddpg_state = next_state.astype(np.float32)\n",
    "        ddpg_agent.remember(ddpg_state, ddpg_norm, rewards[1], next_ddpg_state, done)\n",
    "        ddpg_agent.replay()\n",
    "        \n",
    "        # Decay exploration\n",
    "        if t % 100 == 0:\n",
    "            dqn_agent.update_epsilon()\n",
    "            ddpg_agent.update_epsilon()\n",
    "        \n",
    "        state = next_state\n",
    "        prices_history.append(info['prices'])\n",
    "        profits_history.append(rewards)\n",
    "    \n",
    "    # Calculate averages over last 1000 steps\n",
    "    last_prices = np.array(prices_history[-1000:])\n",
    "    avg_price_dqn = np.mean(last_prices[:, 0])\n",
    "    avg_price_ddpg = np.mean(last_prices[:, 1])\n",
    "    \n",
    "    last_profits = np.array(profits_history[-1000:])\n",
    "    avg_profit_dqn = np.mean(last_profits[:, 0])\n",
    "    avg_profit_ddpg = np.mean(last_profits[:, 1])\n",
    "    \n",
    "    # Get benchmarks\n",
    "    pi_n = benchmarks['E_pi_N']\n",
    "    pi_m = benchmarks['E_pi_M']\n",
    "    p_n = benchmarks['p_N']\n",
    "    p_m = benchmarks['p_M']\n",
    "    \n",
    "    # Calculate Delta (profit-based)\n",
    "    delta_dqn = (avg_profit_dqn - pi_n) / (pi_m - pi_n) if (pi_m - pi_n) != 0 else 0\n",
    "    delta_ddpg = (avg_profit_ddpg - pi_n) / (pi_m - pi_n) if (pi_m - pi_n) != 0 else 0\n",
    "    \n",
    "    # Calculate RPDI (pricing-based)\n",
    "    rpdi_dqn = (avg_price_dqn - p_n) / (p_m - p_n) if (p_m - p_n) != 0 else 0\n",
    "    rpdi_ddpg = (avg_price_ddpg - p_n) / (p_m - p_n) if (p_m - p_n) != 0 else 0\n",
    "    \n",
    "    return avg_price_dqn, avg_price_ddpg, delta_dqn, delta_ddpg, rpdi_dqn, rpdi_ddpg, p_n\n",
    "\n",
    "\n",
    "def main():\n",
    "    shock_cfg = {\n",
    "        'enabled': False\n",
    "    }\n",
    "    \n",
    "    benchmark_calculator = TheoreticalBenchmarks(seed=SEED)\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"DQN vs DDPG - SCHEME NONE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    all_benchmarks = benchmark_calculator.calculate_all_benchmarks(shock_cfg)\n",
    "    \n",
    "    models = ['logit', 'hotelling', 'linear']\n",
    "    results = {}\n",
    "    \n",
    "    for model in models:\n",
    "        print(f\"\\nRunning {model.upper()} simulations...\")\n",
    "        \n",
    "        model_benchmarks = all_benchmarks[model]\n",
    "        \n",
    "        avg_prices_dqn = []\n",
    "        avg_prices_ddpg = []\n",
    "        deltas_dqn = []\n",
    "        deltas_ddpg = []\n",
    "        rpdis_dqn = []\n",
    "        rpdis_ddpg = []\n",
    "        theo_prices = []\n",
    "        \n",
    "        for run in range(NUM_RUNS):\n",
    "            seed = SEED + run\n",
    "            ap_dqn, ap_ddpg, d_dqn, d_ddpg, r_dqn, r_ddpg, p_n = run_simulation(model, seed, shock_cfg, model_benchmarks)\n",
    "            avg_prices_dqn.append(ap_dqn)\n",
    "            avg_prices_ddpg.append(ap_ddpg)\n",
    "            deltas_dqn.append(d_dqn)\n",
    "            deltas_ddpg.append(d_ddpg)\n",
    "            rpdis_dqn.append(r_dqn)\n",
    "            rpdis_ddpg.append(r_ddpg)\n",
    "            theo_prices.append(p_n)\n",
    "        \n",
    "        results[model] = {\n",
    "            'Avg Price DQN': np.mean(avg_prices_dqn),\n",
    "            'Theo Price': np.mean(theo_prices),\n",
    "            'Avg Price DDPG': np.mean(avg_prices_ddpg),\n",
    "            'Delta DQN': np.mean(deltas_dqn),\n",
    "            'Delta DDPG': np.mean(deltas_ddpg),\n",
    "            'RPDI DQN': np.mean(rpdis_dqn),\n",
    "            'RPDI DDPG': np.mean(rpdis_ddpg)\n",
    "        }\n",
    "        \n",
    "        print(f\"  Completed: DQN Δ = {results[model]['Delta DQN']:.3f}, DDPG Δ = {results[model]['Delta DDPG']:.3f}\")\n",
    "    \n",
    "    data = {\n",
    "        'Model': [m.upper() for m in models],\n",
    "        'DQN Avg. Prices': [round(results[m]['Avg Price DQN'], 2) for m in models],\n",
    "        'Theo. Prices': [round(results[m]['Theo Price'], 2) for m in models],\n",
    "        'DDPG Avg. Prices': [round(results[m]['Avg Price DDPG'], 2) for m in models],\n",
    "        'Theo. Prices ': [round(results[m]['Theo Price'], 2) for m in models],\n",
    "        'DQN Extra-profits Δ': [round(results[m]['Delta DQN'], 2) for m in models],\n",
    "        'DDPG Extra-profits Δ': [round(results[m]['Delta DDPG'], 2) for m in models],\n",
    "        'DQN RPDI': [round(results[m]['RPDI DQN'], 2) for m in models],\n",
    "        'DDPG RPDI': [round(results[m]['RPDI DDPG'], 2) for m in models]\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(\"./results/dqn_vs_ddpg.csv\", index=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"FINAL RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    # Overall averages\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"OVERALL AVERAGES ACROSS ALL MODELS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    avg_delta_dqn = np.mean([results[m]['Delta DQN'] for m in models])\n",
    "    avg_delta_ddpg = np.mean([results[m]['Delta DDPG'] for m in models])\n",
    "    avg_rpdi_dqn = np.mean([results[m]['RPDI DQN'] for m in models])\n",
    "    avg_rpdi_ddpg = np.mean([results[m]['RPDI DDPG'] for m in models])\n",
    "    \n",
    "    print(\"\\nDQN:\")\n",
    "    print(f\"  Average Delta (Δ): {avg_delta_dqn:.4f}\")\n",
    "    print(f\"  Average RPDI:      {avg_rpdi_dqn:.4f}\")\n",
    "    print(\"\\nDDPG:\")\n",
    "    print(f\"  Average Delta (Δ): {avg_delta_ddpg:.4f}\")\n",
    "    print(f\"  Average RPDI:      {avg_rpdi_ddpg:.4f}\")\n",
    "    \n",
    "    print(\"\\n[Results saved to ./results/dqn_vs_ddpg.csv]\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3551.088147,
   "end_time": "2025-11-25T15:08:12.673305",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-25T14:09:01.585158",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
