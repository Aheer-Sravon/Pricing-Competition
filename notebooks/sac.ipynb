{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea00974-69f8-4c05-b756-032313e17a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Experience replay buffer for SAC - identical structure to DDPG\"\"\"\n",
    "    def __init__(self, capacity=100000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (np.array(states), np.array(actions), np.array(rewards),\n",
    "                np.array(next_states), np.array(dones))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class SoftQNetwork(nn.Module):\n",
    "    \"\"\"Soft Q-Network for SAC\n",
    "    Source: SAC paper (Haarnoja et al., 2018) Section 4.2\n",
    "    Architecture: Dynamic Pricing paper specifies 3 layers with 128 nodes\"\"\"\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super(SoftQNetwork, self).__init__()\n",
    "        # Dynamic Pricing paper: \"three chained layers with 128 nodes\"\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return self.fc4(x)\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"Stochastic Actor (Policy) Network for SAC\n",
    "    Source: SAC paper Section 4.2, Equation 11-13\n",
    "    Outputs mean and log_std for Gaussian policy with tanh squashing\"\"\"\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128, log_std_min=-20, log_std_max=2):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        self.mean = nn.Linear(hidden_dim, action_dim)\n",
    "        self.log_std = nn.Linear(hidden_dim, action_dim)\n",
    "        \n",
    "        self.log_std_min = log_std_min\n",
    "        self.log_std_max = log_std_max\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        \n",
    "        mean = self.mean(x)\n",
    "        log_std = self.log_std(x)\n",
    "        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)\n",
    "        \n",
    "        return mean, log_std\n",
    "    \n",
    "    def sample(self, state, epsilon=1e-6):\n",
    "        \"\"\"Reparameterization trick for sampling actions\n",
    "        Source: SAC paper Equation 11 and Appendix C\"\"\"\n",
    "        mean, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        \n",
    "        normal = torch.distributions.Normal(mean, std)\n",
    "        z = normal.rsample()  # Reparameterization trick\n",
    "        action = torch.tanh(z)\n",
    "        \n",
    "        # Log probability computation with tanh correction (Appendix C, Eq 21)\n",
    "        log_prob = normal.log_prob(z)\n",
    "        log_prob -= torch.log(1 - action.pow(2) + epsilon)\n",
    "        log_prob = log_prob.sum(1, keepdim=True)\n",
    "        \n",
    "        return action, log_prob, torch.tanh(mean)\n",
    "\n",
    "class SACAgent:\n",
    "    \"\"\"Soft Actor-Critic Agent for pricing competition\n",
    "    Based on: Haarnoja et al. (2018) \"Soft Actor-Critic\"\n",
    "    Hyperparameters from: Dynamic Pricing paper Section 4 & SAC paper Table 1\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        agent_id,\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        hidden_dim=128,  # Dynamic Pricing paper: \"128 nodes\"\n",
    "        actor_lr=3e-4,   # SAC paper Table 1\n",
    "        critic_lr=3e-4,  # SAC paper Table 1\n",
    "        alpha_lr=3e-4,   # For automatic entropy tuning\n",
    "        gamma=0.99,      # SAC paper Table 1\n",
    "        tau=0.005,       # SAC paper Table 1: \"target smoothing coefficient\"\n",
    "        alpha=0.2,       # Initial entropy coefficient\n",
    "        automatic_entropy_tuning=True,\n",
    "        buffer_size=1000000,  # SAC paper: 10^6\n",
    "        batch_size=256,       # SAC paper Table 1\n",
    "        seed=None,\n",
    "        price_min=0.0,\n",
    "        price_max=2.0\n",
    "    ):\n",
    "        self.agent_id = agent_id\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "        self.price_min = price_min\n",
    "        self.price_max = price_max\n",
    "        self.automatic_entropy_tuning = automatic_entropy_tuning\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            random.seed(seed)\n",
    "        \n",
    "        # Actor network (Policy π_φ)\n",
    "        self.actor = PolicyNetwork(state_dim, action_dim, hidden_dim).to(self.device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        \n",
    "        # Two Q-networks (SAC uses two Q-functions to mitigate positive bias)\n",
    "        # Source: SAC paper Section 4.2, following Fujimoto et al. (2018)\n",
    "        self.critic_1 = SoftQNetwork(state_dim, action_dim, hidden_dim).to(self.device)\n",
    "        self.critic_2 = SoftQNetwork(state_dim, action_dim, hidden_dim).to(self.device)\n",
    "        \n",
    "        self.critic_1_optimizer = optim.Adam(self.critic_1.parameters(), lr=critic_lr)\n",
    "        self.critic_2_optimizer = optim.Adam(self.critic_2.parameters(), lr=critic_lr)\n",
    "        \n",
    "        # Target Q-networks (for stable learning)\n",
    "        self.critic_1_target = SoftQNetwork(state_dim, action_dim, hidden_dim).to(self.device)\n",
    "        self.critic_2_target = SoftQNetwork(state_dim, action_dim, hidden_dim).to(self.device)\n",
    "        \n",
    "        self.critic_1_target.load_state_dict(self.critic_1.state_dict())\n",
    "        self.critic_2_target.load_state_dict(self.critic_2.state_dict())\n",
    "        \n",
    "        # Entropy coefficient (alpha) - can be learned automatically\n",
    "        if automatic_entropy_tuning:\n",
    "            self.target_entropy = -torch.prod(torch.Tensor([action_dim]).to(self.device)).item()\n",
    "            self.log_alpha = torch.zeros(1, requires_grad=True, device=self.device)\n",
    "            self.alpha_optimizer = optim.Adam([self.log_alpha], lr=alpha_lr)\n",
    "            self.alpha = self.log_alpha.exp().item()\n",
    "        else:\n",
    "            self.alpha = alpha\n",
    "        \n",
    "        # Replay buffer\n",
    "        self.replay_buffer = ReplayBuffer(buffer_size)\n",
    "        \n",
    "    def select_action(self, state, evaluate=False):\n",
    "        \"\"\"Select action using the current policy\n",
    "        During training: sample from the stochastic policy\n",
    "        During evaluation: use the mean action (deterministic)\"\"\"\n",
    "        \n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        if evaluate:\n",
    "            # Deterministic action (mean of the policy)\n",
    "            _, _, action = self.actor.sample(state)\n",
    "        else:\n",
    "            # Stochastic action (sample from distribution)\n",
    "            action, _, _ = self.actor.sample(state)\n",
    "        \n",
    "        # Convert from [-1, 1] to [price_min, price_max]\n",
    "        action = action.cpu().data.numpy().flatten()\n",
    "        scaled_price = self.price_min + (self.price_max - self.price_min) * (action[0] + 1) / 2\n",
    "        \n",
    "        return scaled_price, action\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store experience in replay buffer\"\"\"\n",
    "        self.replay_buffer.push(state, action, reward, next_state, done)\n",
    "    \n",
    "    def update_parameters(self):\n",
    "        \"\"\"Update actor and critic networks\n",
    "        Source: SAC paper Algorithm 1\"\"\"\n",
    "        \n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # Sample batch from replay buffer\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "        \n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.FloatTensor(actions).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1).to(self.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).unsqueeze(1).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Sample actions from current policy for next states\n",
    "            next_actions, next_log_probs, _ = self.actor.sample(next_states)\n",
    "            \n",
    "            # Compute target Q-values (minimum of two Q-networks for stability)\n",
    "            target_q1 = self.critic_1_target(next_states, next_actions)\n",
    "            target_q2 = self.critic_2_target(next_states, next_actions)\n",
    "            target_q = torch.min(target_q1, target_q2) - self.alpha * next_log_probs\n",
    "            \n",
    "            # Bellman backup: r + γ * (1 - done) * V(s')\n",
    "            target_q = rewards + (1 - dones) * self.gamma * target_q\n",
    "        \n",
    "        # Update critics\n",
    "        current_q1 = self.critic_1(states, actions)\n",
    "        current_q2 = self.critic_2(states, actions)\n",
    "        \n",
    "        critic_1_loss = F.mse_loss(current_q1, target_q)\n",
    "        critic_2_loss = F.mse_loss(current_q2, target_q)\n",
    "        \n",
    "        self.critic_1_optimizer.zero_grad()\n",
    "        critic_1_loss.backward()\n",
    "        self.critic_1_optimizer.step()\n",
    "        \n",
    "        self.critic_2_optimizer.zero_grad()\n",
    "        critic_2_loss.backward()\n",
    "        self.critic_2_optimizer.step()\n",
    "        \n",
    "        # Update actor\n",
    "        new_actions, log_probs, _ = self.actor.sample(states)\n",
    "        q1_new = self.critic_1(states, new_actions)\n",
    "        q2_new = self.critic_2(states, new_actions)\n",
    "        q_new = torch.min(q1_new, q2_new)\n",
    "        \n",
    "        # Actor loss: maximize expected return while maximizing entropy\n",
    "        # J_π = E[α * log π(a|s) - Q(s,a)]\n",
    "        actor_loss = (self.alpha * log_probs - q_new).mean()\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # Update entropy coefficient (if automatic tuning is enabled)\n",
    "        if self.automatic_entropy_tuning:\n",
    "            alpha_loss = -(self.log_alpha * (log_probs + self.target_entropy).detach()).mean()\n",
    "            \n",
    "            self.alpha_optimizer.zero_grad()\n",
    "            alpha_loss.backward()\n",
    "            self.alpha_optimizer.step()\n",
    "            \n",
    "            self.alpha = self.log_alpha.exp().item()\n",
    "        \n",
    "        # Soft update target networks\n",
    "        self._soft_update(self.critic_1, self.critic_1_target)\n",
    "        self._soft_update(self.critic_2, self.critic_2_target)\n",
    "    \n",
    "    def _soft_update(self, local_model, target_model):\n",
    "        \"\"\"Soft update: θ_target = τ*θ_local + (1 - τ)*θ_target\"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(self.tau * local_param.data + (1.0 - self.tau) * target_param.data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
