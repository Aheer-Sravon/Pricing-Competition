{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports and Config","metadata":{}},{"cell_type":"code","source":"import os\nfrom typing import Optional, Dict, Tuple\nimport numpy as np\nimport warnings\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom collections import deque\nimport random\n\nfrom scipy.optimize import minimize_scalar\nimport pandas as pd\n\nSEED = 99\nNUM_RUNS = 50\n\nos.makedirs(\"results\", exist_ok=True)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-26T05:01:26.921063Z","iopub.execute_input":"2025-11-26T05:01:26.922287Z","iopub.status.idle":"2025-11-26T05:01:26.929640Z","shell.execute_reply.started":"2025-11-26T05:01:26.922250Z","shell.execute_reply":"2025-11-26T05:01:26.927953Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# `AR1_Shock`","metadata":{}},{"cell_type":"code","source":"class AR1_Shock:\n    def __init__(self, rho, sigma_eta, seed=None):\n        self.rho = rho\n        self.sigma_eta = sigma_eta\n        self.rng = np.random.RandomState(seed)\n        self.current = 0.0\n\n    def reset(self):\n        self.current = 0.0\n\n    def generate_next(self):\n        eta = self.rng.normal(0, self.sigma_eta)\n        self.current = self.rho * self.current + eta\n        return self.current\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T05:01:26.930796Z","iopub.execute_input":"2025-11-26T05:01:26.931130Z","iopub.status.idle":"2025-11-26T05:01:26.959217Z","shell.execute_reply.started":"2025-11-26T05:01:26.931106Z","shell.execute_reply":"2025-11-26T05:01:26.957712Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# `TheoreticalBenchmarks`","metadata":{}},{"cell_type":"code","source":"class TheoreticalBenchmarks:\n    \"\"\"Calculate theoretical Nash and Monopoly prices/profits for market models\"\"\"\n    \n    def __init__(self, seed=None):\n        self.seed = seed\n        if seed is not None:\n            np.random.seed(seed)\n    \n    def calculate_logit_benchmarks(self, shock_cfg=None):\n        \"\"\"\n        Calculate Nash and Monopoly benchmarks for Logit model\n        With shocks: Uses Monte Carlo integration\n        Without shocks: Uses analytical formulas\n        \"\"\"\n        # Model parameters\n        a = 2.0\n        c = 1.0\n        mu = 0.25\n        a0 = 0.0\n        \n        if shock_cfg is None or not shock_cfg.get('enabled', False):\n            # No shocks - use standard benchmarks\n            p_N = 1.473\n            p_M = 1.925\n            \n            # Calculate profits at these prices\n            exp_N = np.exp((a - p_N) / mu)\n            exp_outside = np.exp(a0 / mu)\n            q_N = exp_N / (2 * exp_N + exp_outside)\n            E_pi_N = (p_N - c) * q_N\n            \n            exp_M = np.exp((a - p_M) / mu)\n            q_M = exp_M / (2 * exp_M + exp_outside)\n            E_pi_M = (p_M - c) * q_M\n            \n            return {\n                'p_N': p_N,\n                'E_pi_N': E_pi_N,\n                'p_M': p_M,\n                'E_pi_M': E_pi_M,\n                'shock_enabled': False\n            }\n        \n        # With shocks - use Monte Carlo\n        scheme_params = {\n            'A': {'rho': 0.3, 'sigma_eta': 0.5},\n            'B': {'rho': 0.95, 'sigma_eta': 0.05},\n            'C': {'rho': 0.9, 'sigma_eta': 0.3}\n        }\n        \n        scheme = shock_cfg.get('scheme', 'A')\n        params = scheme_params[scheme.upper()]\n        rho = params['rho']\n        sigma_eta = params['sigma_eta']\n        \n        # Unconditional variance of shocks\n        sigma2 = sigma_eta**2 / (1 - rho**2)\n        sigma = np.sqrt(sigma2)\n        \n        # Monte Carlo samples\n        N = 10000\n        shock_mode = shock_cfg.get('mode', 'independent')\n        \n        if shock_mode == 'independent':\n            xi_samples = np.random.normal(0, sigma, (N, 2))\n        else:  # correlated\n            xi_samples = np.random.normal(0, sigma, N)\n        \n        # Expected profit functions\n        def E_pi1(p1, p2):\n            if shock_mode == 'independent':\n                exps1 = np.exp((a - p1 + xi_samples[:, 0]) / mu)\n                exps2 = np.exp((a - p2 + xi_samples[:, 1]) / mu)\n            else:\n                exps1 = np.exp((a - p1 + xi_samples) / mu)\n                exps2 = np.exp((a - p2 + xi_samples) / mu)\n            den = exps1 + exps2 + np.exp(a0 / mu)\n            qs = exps1 / den\n            return np.mean((p1 - c) * qs)\n        \n        def E_pi2(p1, p2):\n            if shock_mode == 'independent':\n                exps1 = np.exp((a - p1 + xi_samples[:, 0]) / mu)\n                exps2 = np.exp((a - p2 + xi_samples[:, 1]) / mu)\n            else:\n                exps1 = np.exp((a - p1 + xi_samples) / mu)\n                exps2 = np.exp((a - p2 + xi_samples) / mu)\n            den = exps1 + exps2 + np.exp(a0 / mu)\n            qs = exps2 / den\n            return np.mean((p2 - c) * qs)\n        \n        # Nash equilibrium\n        def best_response(p_j):\n            def neg_E_pi(p_i):\n                return -E_pi1(p_i, p_j)\n            res = minimize_scalar(neg_E_pi, bounds=(c, c + 5), method='bounded')\n            return res.x\n        \n        p_guess = 1.5\n        for _ in range(50):\n            p_guess = best_response(p_guess)\n        p_N = p_guess\n        E_pi_N = E_pi1(p_N, p_N)\n        \n        # Monopoly\n        def neg_E_joint(p):\n            return -(E_pi1(p, p) + E_pi2(p, p))\n        \n        res_M = minimize_scalar(neg_E_joint, bounds=(c, c + 5), method='bounded')\n        p_M = res_M.x\n        E_pi_M = E_pi1(p_M, p_M)\n        \n        return {\n            'p_N': p_N,\n            'E_pi_N': E_pi_N,\n            'p_M': p_M,\n            'E_pi_M': E_pi_M,\n            'shock_enabled': True,\n            'scheme': scheme,\n            'mode': shock_mode,\n            'sigma': sigma\n        }\n    \n    def calculate_hotelling_benchmarks(self, shock_cfg=None):\n        \"\"\"\n        Calculate Nash and Monopoly benchmarks for Hotelling model\n        Shocks don't affect expected benchmarks (linearity in net shock)\n        \"\"\"\n        # Model parameters\n        c = 0.0\n        v_bar = 1.75\n        theta = 1.0\n        \n        # Standard benchmarks (unchanged with shocks)\n        p_N = 1.00\n        p_M = 1.25\n        \n        # Calculate profits\n        # At Nash: both firms charge 1, split market equally\n        q_N = 0.5\n        E_pi_N = p_N * q_N\n        \n        # At Monopoly: both charge 1.25, split market equally\n        q_M = 0.5\n        E_pi_M = p_M * q_M\n        \n        return {\n            'p_N': p_N,\n            'E_pi_N': E_pi_N,\n            'p_M': p_M,\n            'E_pi_M': E_pi_M,\n            'shock_enabled': shock_cfg is not None and shock_cfg.get('enabled', False),\n            'note': 'Shocks do not affect expected benchmarks for Hotelling'\n        }\n    \n    def calculate_linear_benchmarks(self, shock_cfg=None):\n        \"\"\"\n        Calculate Nash and Monopoly benchmarks for Linear model\n        Shocks don't affect expected benchmarks (linearity in shocks)\n        \"\"\"\n        # Model parameters\n        c = 0.0\n        a_bar = 1.0\n        d = 0.25\n        \n        # Standard benchmarks (unchanged with shocks)\n        p_N = 0.4286\n        p_M = 0.5\n        \n        # Calculate profits\n        denominator = 1 - d**2\n        \n        # At Nash\n        q_N = (a_bar - p_N - d * (a_bar - p_N)) / denominator\n        E_pi_N = p_N * q_N\n        \n        # At Monopoly\n        q_M = a_bar - p_M\n        E_pi_M = p_M * q_M\n        \n        return {\n            'p_N': p_N,\n            'E_pi_N': E_pi_N,\n            'p_M': p_M,\n            'E_pi_M': E_pi_M,\n            'shock_enabled': shock_cfg is not None and shock_cfg.get('enabled', False),\n            'note': 'Shocks do not affect expected benchmarks for Linear'\n        }\n    \n    def calculate_all_benchmarks(self, shock_cfg=None):\n        \"\"\"Calculate benchmarks for all three models\"\"\"\n        results = {\n            'logit': self.calculate_logit_benchmarks(shock_cfg),\n            'hotelling': self.calculate_hotelling_benchmarks(shock_cfg),\n            'linear': self.calculate_linear_benchmarks(shock_cfg)\n        }\n        return results\n    \n    def generate_benchmark_table(self, shock_configs):\n        \"\"\"\n        Generate a comprehensive table of benchmarks for different configurations\n        \n        Args:\n            shock_configs: List of shock configurations to test\n        \n        Returns:\n            pandas DataFrame with benchmarks\n        \"\"\"\n        data = []\n        \n        for config in shock_configs:\n            config_name = config.get('name', 'No Config')\n            benchmarks = self.calculate_all_benchmarks(config)\n            \n            for model, bench in benchmarks.items():\n                data.append({\n                    'Configuration': config_name,\n                    'Model': model.upper(),\n                    'Nash Price': round(bench['p_N'], 4),\n                    'Nash Profit': round(bench['E_pi_N'], 4),\n                    'Monopoly Price': round(bench['p_M'], 4),\n                    'Monopoly Profit': round(bench['E_pi_M'], 4),\n                    'Shock Enabled': bench['shock_enabled']\n                })\n        \n        return pd.DataFrame(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T05:01:26.960012Z","iopub.execute_input":"2025-11-26T05:01:26.960303Z","iopub.status.idle":"2025-11-26T05:01:26.992362Z","shell.execute_reply.started":"2025-11-26T05:01:26.960281Z","shell.execute_reply":"2025-11-26T05:01:26.990900Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# `MarketEnv` Class","metadata":{}},{"cell_type":"code","source":"class MarketEnv:\n    \"\"\"Base Market environment supporting all three models with optional shocks\"\"\"\n   \n    def __init__(\n        self,\n        market_model: str = \"logit\",\n        shock_cfg: Optional[Dict] = None,\n        n_firms: int = 2,\n        horizon: int = 10000,\n        seed: Optional[int] = None\n    ):\n        self.model = market_model.lower()\n        self.n_firms = n_firms\n        self.N = 15 # Number of discrete prices\n        self.horizon = horizon\n        self.t = 0\n        self.rng = np.random.RandomState(seed)\n       \n        # Model-specific parameters (verified against PDFs)\n        if self.model == \"logit\":\n            self.c = 1.0\n            self.a = 2.0\n            self.a0 = 0.0\n            self.mu = 0.25\n            self.P_N = 1.473\n            self.P_M = 1.925\n        elif self.model == \"hotelling\":\n            self.c = 0.0\n            self.v_bar = 1.75\n            self.theta = 1.0\n            self.P_N = 1.00\n            self.P_M = 1.25\n        elif self.model == \"linear\":\n            self.c = 0.0\n            self.a_bar = 1.0\n            self.d = 0.25\n            \n            # Nash (Cournot duopoly)\n            self.P_N = self.a_bar * (1 - self.d) / (2 - self.d)  # 0.4286\n            q_N = self.P_N  # By symmetry at equilibrium\n            pi_N = self.P_N * q_N  # 0.1959\n            \n            # Monopoly (single firm, no competition)\n            self.P_M = self.a_bar / 2  # 0.5\n            q_M = self.a_bar - self.P_M  # 0.5 (residual demand, NOT Cournot!)\n            pi_M = self.P_M * q_M  # 0.25\n            \n            # Store for Delta calculations\n            self.pi_N_linear = pi_N  # 0.1959\n            self.pi_M_linear = pi_M  # 0.25\n            \n            # Verify profit range is healthy\n            profit_range = pi_M - pi_N  # Should be ~0.05\n            if profit_range < 0.04:\n                import warnings\n                warnings.warn(f\"Linear model profit range too small: {profit_range:.4f}\")\n        else:\n            raise ValueError(f\"Unknown model: {self.model}\")\n       \n        # Construct price grid\n        span = self.P_M - self.P_N\n        self.price_grid = np.linspace(\n            self.P_N - 0.15 * span,\n            self.P_M + 0.15 * span,\n            self.N\n        )\n       \n        # Initialize shock configuration\n        self.shock_cfg = shock_cfg or {}\n        self.shock_enabled = self.shock_cfg.get(\"enabled\", False)\n       \n        if self.shock_enabled:\n            self.shock_mode = self.shock_cfg.get(\"mode\", \"correlated\")\n            scheme = self.shock_cfg.get(\"scheme\", None)\n           \n            # Get AR(1) parameters (verified against PDFs)\n            if scheme:\n                scheme_params = {\n                    'A': {'rho': 0.3, 'sigma_eta': 0.5},\n                    'B': {'rho': 0.95, 'sigma_eta': 0.05},\n                    'C': {'rho': 0.9, 'sigma_eta': 0.3}\n                }\n                if scheme.upper() in scheme_params:\n                    params = scheme_params[scheme.upper()]\n                    self.rho = params['rho']\n                    self.sigma_eta = params['sigma_eta']\n                else:\n                    raise ValueError(f\"Unknown scheme: {scheme}\")\n            else:\n                self.rho = self.shock_cfg.get('rho', 0.9)\n                self.sigma_eta = self.shock_cfg.get('sigma_eta', 0.15)\n           \n            # Initialize shock generators\n            if self.shock_mode == \"independent\":\n                self.shock_generators = [\n                    AR1_Shock(self.rho, self.sigma_eta, seed=seed+i if seed else None)\n                    for i in range(self.n_firms)\n                ]\n            else: # correlated\n                self.shock_generator = AR1_Shock(self.rho, self.sigma_eta, seed=seed)\n           \n            self.current_shocks = np.zeros(self.n_firms)\n       \n        self.reset()\n   \n    def reset(self):\n        \"\"\"Reset environment - returns state as actual prices\"\"\"\n        self.t = 0\n       \n        # Reset shocks\n        if self.shock_enabled:\n            if self.shock_mode == \"independent\":\n                for gen in self.shock_generators:\n                    gen.reset()\n            else:\n                self.shock_generator.reset()\n            self.current_shocks = np.zeros(self.n_firms)\n       \n        # Initialize prices at middle of grid\n        mid_idx = self.N // 2\n        self.current_price_idx = np.array([mid_idx] * self.n_firms)\n       \n        return self._get_state()\n   \n    def _get_state(self):\n        \"\"\"Get current state as actual prices (numpy array)\"\"\"\n        return self.price_grid[self.current_price_idx].copy()\n    \n    def _get_state_indices(self):\n        \"\"\"Get current state as price indices (for Q-learning compatibility)\"\"\"\n        return tuple(self.current_price_idx)\n   \n    def _generate_shocks(self):\n        \"\"\"Generate next period shocks\"\"\"\n        if not self.shock_enabled:\n            return np.zeros(self.n_firms)\n       \n        if self.shock_mode == \"independent\":\n            shocks = np.array([gen.generate_next() for gen in self.shock_generators])\n        else:\n            shock = self.shock_generator.generate_next()\n            shocks = np.array([shock] * self.n_firms)\n       \n        return shocks\n   \n    def get_current_shocks(self):\n        \"\"\"Get current shocks (for PSO evaluation)\"\"\"\n        return self.current_shocks.copy() if self.shock_enabled else np.zeros(self.n_firms)\n   \n    def calculate_demand_and_profit(self, prices: np.ndarray, shocks: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Calculate demand and profit given prices and shocks\"\"\"\n        if self.model == \"logit\":\n            return self._logit_demand_profit(prices, shocks)\n        elif self.model == \"hotelling\":\n            return self._hotelling_demand_profit(prices, shocks)\n        elif self.model == \"linear\":\n            return self._linear_demand_profit(prices, shocks)\n   \n    def _logit_demand_profit(self, prices: np.ndarray, shocks: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Logit demand - shocks affect variance but not Nash equilibrium\n        \n        Critical: E[demand | shocks] = demand(no shocks) since E[ε] = 0\n        Nash price must remain constant regardless of shock scheme\n        \"\"\"\n        # Base utilities WITHOUT shocks (defines Nash equilibrium)\n        base_utilities = (self.a - prices) / self.mu\n        \n        # Add shocks OUTSIDE mu scaling to preserve E[utility]\n        realized_utilities = base_utilities + shocks\n        \n        # Numerical stability\n        max_util = np.max(realized_utilities)\n        exp_utils = np.exp(realized_utilities - max_util)\n        exp_outside = np.exp(self.a0/self.mu - max_util)\n        \n        denominator = np.sum(exp_utils) + exp_outside\n        demands = exp_utils / denominator\n        profits = (prices - self.c) * demands\n        \n        return demands, profits\n   \n    def _hotelling_demand_profit(self, prices: np.ndarray, shocks: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Hotelling demand with net shock affecting boundary (verified against PDF)\"\"\"\n        p1, p2 = prices[0], prices[1]\n        epsilon_net = shocks[0] - shocks[1] if self.n_firms == 2 else 0\n       \n        x_hat = 0.5 + (p2 - p1 + epsilon_net) / (2 * self.theta)\n        x_hat = np.clip(x_hat, 0, 1)\n       \n        q1 = x_hat\n        q2 = 1 - x_hat\n        demands = np.array([q1, q2])\n        profits = prices * demands\n       \n        return demands, profits\n   \n    def _linear_demand_profit(self, prices: np.ndarray, shocks: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Linear demand with additive shocks (verified against PDF)\"\"\"\n        a_shocked = self.a_bar + shocks\n        denominator = 1 - self.d**2\n       \n        q1 = ((a_shocked[0] - prices[0]) - self.d * (a_shocked[1] - prices[1])) / denominator\n        q2 = ((a_shocked[1] - prices[1]) - self.d * (a_shocked[0] - prices[0])) / denominator\n       \n        q1 = max(0, q1)\n        q2 = max(0, q2)\n        demands = np.array([q1, q2])\n        profits = prices * demands\n       \n        return demands, profits\n   \n    def step(self, action_indices):\n        \"\"\"Execute one step - returns (state_prices, profits, done, info)\n        \n        Args:\n            action_indices: Array of price indices for each agent\n            \n        Returns:\n            next_state: Numpy array of actual prices (not indices)\n            profits: Numpy array of profits for each agent\n            done: Episode termination flag\n            info: Dictionary with prices, demands, and shocks\n        \"\"\"\n        action_indices = np.asarray(action_indices, dtype=int)\n       \n        # Update price indices\n        self.current_price_idx = action_indices\n        prices = self.price_grid[self.current_price_idx]\n       \n        # Generate shocks for this period\n        self.current_shocks = self._generate_shocks()\n       \n        # Calculate demands and profits\n        demands, profits = self.calculate_demand_and_profit(prices, self.current_shocks)\n       \n        # Update time\n        self.t += 1\n       \n        # Get next state (actual prices, not indices)\n        next_state = self._get_state()\n       \n        # Episode termination\n        done = False\n       \n        # Info dictionary\n        info = {\n            'prices': prices.copy(),\n            'demands': demands.copy(),\n            'shocks': self.current_shocks.copy(),\n            'price_indices': self.current_price_idx.copy()  # Include indices for Q-learning\n        }\n       \n        return next_state, profits, done, info\n\nclass MarketEnvContinuous(MarketEnv):\n    \"\"\"Market environment that accepts both discrete indices and continuous prices as actions\"\"\"\n    \n    def step(self, actions):\n        \"\"\"Execute one step with flexible action handling\n        \n        Args:\n            actions: List/array where each element can be:\n                     - int/np.integer: discrete price index\n                     - float: continuous price value\n                     \n        Returns:\n            next_state: Numpy array of actual prices\n            profits: Numpy array of profits\n            done: Episode termination flag\n            info: Dictionary with prices, demands, shocks, and indices\n        \"\"\"\n        prices = []\n        indices = []\n        \n        for a in actions:\n            if isinstance(a, (int, np.integer)):\n                # Discrete action: use price from grid\n                prices.append(self.price_grid[a])\n                indices.append(a)\n            else:\n                # Continuous action: use actual price value\n                price = float(a)\n                prices.append(price)\n                # Find closest grid index for tracking\n                indices.append(np.argmin(np.abs(self.price_grid - price)))\n        \n        prices = np.array(prices)\n        self.current_price_idx = np.array(indices)\n        \n        # Generate shocks\n        self.current_shocks = self._generate_shocks()\n        \n        # Calculate demands and profits\n        demands, profits = self.calculate_demand_and_profit(prices, self.current_shocks)\n        \n        # Update time\n        self.t += 1\n        \n        # Get next state (actual prices)\n        next_state = self._get_state()\n        \n        # Episode termination\n        done = False\n        \n        # Info dictionary\n        info = {\n            'prices': prices.copy(),\n            'demands': demands.copy(),\n            'shocks': self.current_shocks.copy(),\n            'price_indices': self.current_price_idx.copy()\n        }\n        \n        return next_state, profits, done, info\n\n    def get_state_prices(self):\n        \"\"\"Get current state as actual prices (same as _get_state)\"\"\"\n        return self._get_state()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T05:01:27.040727Z","iopub.execute_input":"2025-11-26T05:01:27.041910Z","iopub.status.idle":"2025-11-26T05:01:27.074787Z","shell.execute_reply.started":"2025-11-26T05:01:27.041869Z","shell.execute_reply":"2025-11-26T05:01:27.073392Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# `QLearningAgent`","metadata":{}},{"cell_type":"code","source":"class QLearningAgent:\n    def __init__(self, n_actions, alpha=0.15, gamma=0.95, beta=1.5e-4, agent_id=0, price_grid=None):\n        \"\"\"\n        Q-Learning Agent for discrete pricing decisions.\n        \n        Args:\n            n_actions: Number of discrete price points\n            alpha: Learning rate\n            gamma: Discount factor\n            beta: Exploration decay rate\n            agent_id: Agent identifier (0 or 1)\n            price_grid: Price grid for converting continuous states to indices (optional)\n        \"\"\"\n        self.Q = np.zeros((n_actions, n_actions))\n        self.alpha = alpha  # Learning rate\n        self.gamma = gamma  # Discount factor\n        self.beta = beta  # Exploration decay rate\n        self.t = 0\n        self.id = agent_id\n        self.n_actions = n_actions\n        self.price_grid = price_grid  # Store price grid for state conversion\n    \n    def _state_to_indices(self, state):\n        \"\"\"Convert continuous state (prices) to discrete indices\n        \n        Args:\n            state: Can be either:\n                   - tuple of indices (old format, backward compatible)\n                   - numpy array of prices (new format)\n                   \n        Returns:\n            tuple of indices\n        \"\"\"\n        if isinstance(state, tuple):\n            # Already indices (backward compatible)\n            return state\n        elif isinstance(state, np.ndarray):\n            # Convert prices to indices\n            if self.price_grid is not None:\n                indices = tuple(np.argmin(np.abs(self.price_grid - price)) for price in state)\n                return indices\n            else:\n                # Fallback: assume state is small integers that can be indices\n                return tuple(state.astype(int))\n        else:\n            raise ValueError(f\"Unknown state format: {type(state)}\")\n\n    def choose_action(self, state):\n        \"\"\"Choose action using epsilon-greedy with exponential decay\n        \n        Args:\n            state: Either tuple of indices or numpy array of prices\n            \n        Returns:\n            action: Discrete action index\n        \"\"\"\n        state_idx = self._state_to_indices(state)\n        opp_idx = state_idx[1 - self.id]\n        \n        epsilon = np.exp(-self.beta * self.t)\n        if np.random.rand() < epsilon:\n            return np.random.randint(self.n_actions)  # Explore\n        else:\n            return np.argmax(self.Q[opp_idx])  # Exploit\n\n    def update(self, state, action, reward, next_state):\n        \"\"\"Update Q-table using TD learning\n        \n        Args:\n            state: Current state (indices or prices)\n            action: Action taken\n            reward: Reward received\n            next_state: Next state (indices or prices)\n        \"\"\"\n        state_idx = self._state_to_indices(state)\n        next_state_idx = self._state_to_indices(next_state)\n        \n        opp_idx = state_idx[1 - self.id]\n        next_opp_idx = next_state_idx[1 - self.id]\n        \n        td_target = reward + self.gamma * np.max(self.Q[next_opp_idx])\n        self.Q[opp_idx, action] += self.alpha * (td_target - self.Q[opp_idx, action])\n        self.t += 1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T05:01:27.076688Z","iopub.execute_input":"2025-11-26T05:01:27.077792Z","iopub.status.idle":"2025-11-26T05:01:27.103420Z","shell.execute_reply.started":"2025-11-26T05:01:27.077760Z","shell.execute_reply":"2025-11-26T05:01:27.102378Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# `DDPGAgent`","metadata":{}},{"cell_type":"code","source":"class ReplayBuffer:\n    \"\"\"Experience replay buffer for DDPG\"\"\"\n    def __init__(self, capacity=100000):\n        self.buffer = deque(maxlen=capacity)\n   \n    def push(self, state, action, reward, next_state, done):\n        self.buffer.append((state, action, reward, next_state, done))\n   \n    def sample(self, batch_size):\n        batch = random.sample(self.buffer, batch_size)\n        states, actions, rewards, next_states, dones = zip(*batch)\n        return (np.array(states), np.array(actions), np.array(rewards),\n                np.array(next_states), np.array(dones))\n   \n    def __len__(self):\n        return len(self.buffer)\n\nclass Actor(nn.Module):\n    \"\"\"Actor network for DDPG\"\"\"\n    def __init__(self, state_dim, action_dim, hidden_dim=400):\n        super(Actor, self).__init__()\n        self.bn_input = nn.BatchNorm1d(state_dim)\n        self.fc1 = nn.Linear(state_dim, hidden_dim)\n        self.bn1 = nn.BatchNorm1d(hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, 300)\n        self.bn2 = nn.BatchNorm1d(300)\n        self.fc3 = nn.Linear(300, action_dim)\n       \n        # Initialize weights\n        self._init_weights()\n   \n    def _init_weights(self):\n        nn.init.uniform_(self.fc1.weight, -1/np.sqrt(self.fc1.in_features), 1/np.sqrt(self.fc1.in_features))\n        nn.init.uniform_(self.fc2.weight, -1/np.sqrt(self.fc2.in_features), 1/np.sqrt(self.fc2.in_features))\n        nn.init.uniform_(self.fc3.weight, -3e-3, 3e-3)\n   \n    def forward(self, state):\n        x = self.bn_input(state)\n        x = F.relu(self.bn1(self.fc1(x)))\n        x = F.relu(self.bn2(self.fc2(x)))\n        x = torch.tanh(self.fc3(x))\n        return x\n\nclass Critic(nn.Module):\n    \"\"\"Critic network for DDPG\"\"\"\n    def __init__(self, state_dim, action_dim, hidden_dim=400):\n        super(Critic, self).__init__()\n        self.bn_input = nn.BatchNorm1d(state_dim)\n        self.fc1 = nn.Linear(state_dim, hidden_dim)\n        self.bn1 = nn.BatchNorm1d(hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim + action_dim, 300)\n        self.fc3 = nn.Linear(300, 1)\n       \n        # Initialize weights\n        self._init_weights()\n   \n    def _init_weights(self):\n        nn.init.uniform_(self.fc1.weight, -1/np.sqrt(self.fc1.in_features), 1/np.sqrt(self.fc1.in_features))\n        nn.init.uniform_(self.fc2.weight, -1/np.sqrt(self.fc2.in_features), 1/np.sqrt(self.fc2.in_features))\n        nn.init.uniform_(self.fc3.weight, -3e-3, 3e-3)\n   \n    def forward(self, state, action):\n        x = self.bn_input(state)\n        x = F.relu(self.bn1(self.fc1(x)))\n        x = torch.cat([x, action], dim=1)\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nclass OUNoise:\n    \"\"\"Ornstein-Uhlenbeck process for exploration\"\"\"\n    def __init__(self, action_dim, mu=0.0, theta=0.15, sigma=0.2):\n        self.action_dim = action_dim\n        self.mu = mu\n        self.theta = theta\n        self.sigma = sigma\n        self.state = np.ones(self.action_dim) * self.mu\n        self.reset()\n   \n    def reset(self):\n        self.state = np.ones(self.action_dim) * self.mu\n   \n    def sample(self):\n        dx = self.theta * (self.mu - self.state)\n        dx += self.sigma * np.random.randn(self.action_dim)\n        self.state += dx\n        return self.state.copy()\n\nclass DDPGAgent:\n    \"\"\"Deep Deterministic Policy Gradient Agent for pricing competition simulation\"\"\"\n    def __init__(\n        self,\n        agent_id,\n        state_dim,\n        action_dim,\n        hidden_dim=400,\n        actor_lr=1e-4,\n        critic_lr=1e-3,\n        gamma=0.99,\n        tau=0.001,\n        buffer_size=1000000,\n        batch_size=64,\n        seed=None,\n        price_min=0.0,\n        price_max=2.0\n    ):\n        self.agent_id = agent_id\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.gamma = gamma\n        self.tau = tau\n        self.batch_size = batch_size\n        self.price_min = price_min\n        self.price_max = price_max\n        \n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        \n        if seed is not None:\n            torch.manual_seed(seed)\n            if self.device.type == 'cuda':\n                torch.cuda.manual_seed(seed)\n            np.random.seed(seed)\n            random.seed(seed)\n        \n        # For deterministic behavior on GPU\n        if self.device.type == 'cuda':\n            torch.backends.cudnn.deterministic = True\n            torch.backends.cudnn.benchmark = False\n        \n        # Actor networks\n        self.actor = Actor(state_dim, action_dim, hidden_dim).to(self.device)\n        self.actor_target = Actor(state_dim, action_dim, hidden_dim).to(self.device)\n        self.actor_target.load_state_dict(self.actor.state_dict())\n        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n        \n        # Critic networks\n        self.critic = Critic(state_dim, action_dim, hidden_dim).to(self.device)\n        self.critic_target = Critic(state_dim, action_dim, hidden_dim).to(self.device)\n        self.critic_target.load_state_dict(self.critic.state_dict())\n        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr, weight_decay=1e-2)\n        \n        # Replay buffer\n        self.replay_buffer = ReplayBuffer(buffer_size)\n        \n        # Noise process\n        self.noise = OUNoise(action_dim)\n        \n        # Exploration parameters\n        self.epsilon = 1.0\n        self.epsilon_min = 0.01\n        self.epsilon_decay = 0.995\n   \n    def select_action(self, state, explore=True):\n        \"\"\"Select action using actor network with optional exploration noise\"\"\"\n        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n        self.actor.eval()\n        with torch.no_grad():\n            action = self.actor(state).cpu().numpy()[0]\n        self.actor.train()\n        \n        if explore:\n            noise = self.noise.sample() * self.epsilon\n            action += noise\n            action = np.clip(action, -1, 1)\n        \n        normalized_action = action.copy()  # For remember/replay (normalized [-1,1])\n        \n        # Scale to [price_min, price_max]\n        scaled_price = self.price_min + (self.price_max - self.price_min) * (action[0] + 1) / 2\n        \n        return scaled_price, normalized_action\n   \n    def remember(self, state, action, reward, next_state, done):\n        \"\"\"Store experience in replay buffer\"\"\"\n        self.replay_buffer.push(state, action, reward, next_state, done)\n   \n    def replay(self):\n        \"\"\"Train on a batch of experiences from replay buffer\"\"\"\n        if len(self.replay_buffer) < self.batch_size:\n            return\n       \n        # Sample batch\n        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n       \n        states = torch.FloatTensor(states).to(self.device)\n        actions = torch.FloatTensor(actions).to(self.device)\n        rewards = torch.FloatTensor(rewards).unsqueeze(1).to(self.device)\n        next_states = torch.FloatTensor(next_states).to(self.device)\n        dones = torch.FloatTensor(dones).unsqueeze(1).to(self.device)\n       \n        # Update critic\n        with torch.no_grad():\n            next_actions = self.actor_target(next_states)\n            target_q = self.critic_target(next_states, next_actions)\n            target_q = rewards + (1 - dones) * self.gamma * target_q\n       \n        current_q = self.critic(states, actions)\n        critic_loss = F.mse_loss(current_q, target_q)\n       \n        self.critic_optimizer.zero_grad()\n        critic_loss.backward()\n        self.critic_optimizer.step()\n       \n        # Update actor\n        actor_loss = -self.critic(states, self.actor(states)).mean()\n       \n        self.actor_optimizer.zero_grad()\n        actor_loss.backward()\n        self.actor_optimizer.step()\n       \n        # Soft update target networks\n        self._soft_update(self.actor, self.actor_target)\n        self._soft_update(self.critic, self.critic_target)\n   \n    def _soft_update(self, local_model, target_model):\n        \"\"\"Soft update target network parameters: θ_target = τ*θ_local + (1 - τ)*θ_target\"\"\"\n        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n            target_param.data.copy_(self.tau * local_param.data + (1.0 - self.tau) * target_param.data)\n   \n    def update_epsilon(self):\n        \"\"\"Decay exploration rate\"\"\"\n        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n   \n    def reset_noise(self):\n        \"\"\"Reset the noise process\"\"\"\n        self.noise.reset()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T05:01:27.104522Z","iopub.execute_input":"2025-11-26T05:01:27.104813Z","iopub.status.idle":"2025-11-26T05:01:27.139030Z","shell.execute_reply.started":"2025-11-26T05:01:27.104783Z","shell.execute_reply":"2025-11-26T05:01:27.137753Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# Simulation","metadata":{}},{"cell_type":"code","source":"def run_simulation(model, seed, shock_cfg, benchmarks):\n    \"\"\"Run Q-Learning vs DDPG simulation\"\"\"\n    np.random.seed(seed)\n    \n    env = MarketEnvContinuous(market_model=model, shock_cfg=shock_cfg, seed=seed)\n    \n    price_min = env.price_grid.min()\n    price_max = env.price_grid.max()\n    \n    q_agent = QLearningAgent(env.N, agent_id=0, price_grid=env.price_grid)\n    ddpg_agent = DDPGAgent(\n        agent_id=1,\n        state_dim=2,\n        action_dim=1,\n        seed=seed,\n        price_min=price_min,\n        price_max=price_max\n    )\n    \n    state = env.reset()\n    profits_history = []\n    prices_history = []\n    \n    for t in range(env.horizon):\n        # Q-Learning selects discrete action index\n        q_action_idx = q_agent.choose_action(state)\n        q_price = env.price_grid[q_action_idx]\n        \n        # DDPG selects continuous price\n        ddpg_state = state.astype(np.float32)\n        ddpg_price, ddpg_norm = ddpg_agent.select_action(ddpg_state, explore=True)\n        \n        actions = [q_price, ddpg_price]\n        next_state, rewards, done, info = env.step(actions)\n        \n        # Update Q-Learning\n        q_agent.update(state, q_action_idx, rewards[0], next_state)\n        \n        # Update DDPG\n        next_ddpg_state = next_state.astype(np.float32)\n        ddpg_agent.remember(ddpg_state, ddpg_norm, rewards[1], next_ddpg_state, done)\n        ddpg_agent.replay()\n        \n        if t % 100 == 0:\n            ddpg_agent.update_epsilon()\n        \n        state = next_state\n        prices_history.append(info['prices'])\n        profits_history.append(rewards)\n    \n    # Calculate averages over last 1000 steps\n    last_prices = np.array(prices_history[-1000:])\n    avg_price_q = np.mean(last_prices[:, 0])\n    avg_price_ddpg = np.mean(last_prices[:, 1])\n    \n    last_profits = np.array(profits_history[-1000:])\n    avg_profit_q = np.mean(last_profits[:, 0])\n    avg_profit_ddpg = np.mean(last_profits[:, 1])\n    \n    # Get benchmarks\n    pi_n = benchmarks['E_pi_N']\n    pi_m = benchmarks['E_pi_M']\n    p_n = benchmarks['p_N']\n    p_m = benchmarks['p_M']\n    \n    # Calculate Delta (profit-based)\n    delta_q = (avg_profit_q - pi_n) / (pi_m - pi_n) if (pi_m - pi_n) != 0 else 0\n    delta_ddpg = (avg_profit_ddpg - pi_n) / (pi_m - pi_n) if (pi_m - pi_n) != 0 else 0\n    \n    # Calculate RPDI (pricing-based)\n    rpdi_q = (avg_price_q - p_n) / (p_m - p_n) if (p_m - p_n) != 0 else 0\n    rpdi_ddpg = (avg_price_ddpg - p_n) / (p_m - p_n) if (p_m - p_n) != 0 else 0\n    \n    return avg_price_q, avg_price_ddpg, delta_q, delta_ddpg, rpdi_q, rpdi_ddpg, p_n\n\n\ndef main():\n    shock_cfg = {\n        'enabled': True,\n        'scheme': 'B',\n        'mode': 'independent'\n    }\n    \n    benchmark_calculator = TheoreticalBenchmarks(seed=SEED)\n    \n    print(\"=\" * 80)\n    print(\"Q-LEARNING vs DDPG - SCHEME B\")\n    print(\"=\" * 80)\n    \n    all_benchmarks = benchmark_calculator.calculate_all_benchmarks(shock_cfg)\n    \n    models = ['logit', 'hotelling', 'linear']\n    results = {}\n    \n    for model in models:\n        print(f\"\\nRunning {model.upper()} simulations...\")\n        \n        model_benchmarks = all_benchmarks[model]\n        \n        avg_prices_q = []\n        avg_prices_ddpg = []\n        deltas_q = []\n        deltas_ddpg = []\n        rpdis_q = []\n        rpdis_ddpg = []\n        theo_prices = []\n        \n        for run in range(NUM_RUNS):\n            seed = SEED + run\n            apq, apd, dq, dd, rq, rd, p_n = run_simulation(model, seed, shock_cfg, model_benchmarks)\n            avg_prices_q.append(apq)\n            avg_prices_ddpg.append(apd)\n            deltas_q.append(dq)\n            deltas_ddpg.append(dd)\n            rpdis_q.append(rq)\n            rpdis_ddpg.append(rd)\n            theo_prices.append(p_n)\n        \n        results[model] = {\n            'Avg Price Q': np.mean(avg_prices_q),\n            'Theo Price': np.mean(theo_prices),\n            'Avg Price DDPG': np.mean(avg_prices_ddpg),\n            'Delta Q': np.mean(deltas_q),\n            'Delta DDPG': np.mean(deltas_ddpg),\n            'RPDI Q': np.mean(rpdis_q),\n            'RPDI DDPG': np.mean(rpdis_ddpg)\n        }\n        \n        print(f\"  Completed: Q Δ = {results[model]['Delta Q']:.3f}, DDPG Δ = {results[model]['Delta DDPG']:.3f}\")\n    \n    data = {\n        'Model': [m.upper() for m in models],\n        'Q Avg. Prices': [round(results[m]['Avg Price Q'], 2) for m in models],\n        'Theo. Prices': [round(results[m]['Theo Price'], 2) for m in models],\n        'DDPG Avg. Prices': [round(results[m]['Avg Price DDPG'], 2) for m in models],\n        'Theo. Prices ': [round(results[m]['Theo Price'], 2) for m in models],\n        'Q Extra-profits Δ': [round(results[m]['Delta Q'], 2) for m in models],\n        'DDPG Extra-profits Δ': [round(results[m]['Delta DDPG'], 2) for m in models],\n        'Q RPDI': [round(results[m]['RPDI Q'], 2) for m in models],\n        'DDPG RPDI': [round(results[m]['RPDI DDPG'], 2) for m in models]\n    }\n    \n    df = pd.DataFrame(data)\n    df.to_csv(\"./results/q_vs_ddpg.csv\", index=False)\n    \n    print(\"\\n\" + \"=\" * 80)\n    print(\"FINAL RESULTS\")\n    print(\"=\" * 80)\n    print(df.to_string(index=False))\n    \n    # Overall averages\n    print(\"\\n\" + \"=\" * 80)\n    print(\"OVERALL AVERAGES ACROSS ALL MODELS\")\n    print(\"=\" * 80)\n    \n    avg_delta_q = np.mean([results[m]['Delta Q'] for m in models])\n    avg_delta_ddpg = np.mean([results[m]['Delta DDPG'] for m in models])\n    avg_rpdi_q = np.mean([results[m]['RPDI Q'] for m in models])\n    avg_rpdi_ddpg = np.mean([results[m]['RPDI DDPG'] for m in models])\n    \n    print(\"\\nQ-Learning:\")\n    print(f\"  Average Delta (Δ): {avg_delta_q:.4f}\")\n    print(f\"  Average RPDI:      {avg_rpdi_q:.4f}\")\n    print(\"\\nDDPG:\")\n    print(f\"  Average Delta (Δ): {avg_delta_ddpg:.4f}\")\n    print(f\"  Average RPDI:      {avg_rpdi_ddpg:.4f}\")\n    \n    print(\"\\n[Results saved to ./results/q_vs_ddpg.csv]\")\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T05:01:27.157080Z","iopub.execute_input":"2025-11-26T05:01:27.157434Z","iopub.status.idle":"2025-11-26T05:01:38.154603Z","shell.execute_reply.started":"2025-11-26T05:01:27.157408Z","shell.execute_reply":"2025-11-26T05:01:38.152873Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nQ-LEARNING vs DDPG - SCHEME B\n================================================================================\n\nRunning LOGIT simulations...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/3629253460.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_47/3629253460.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrun\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_RUNS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSEED\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0mapq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_simulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshock_cfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_benchmarks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m             \u001b[0mavg_prices_q\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mavg_prices_ddpg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/3629253460.py\u001b[0m in \u001b[0;36mrun_simulation\u001b[0;34m(model, seed, shock_cfg, benchmarks)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mnext_ddpg_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mddpg_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mddpg_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mddpg_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_ddpg_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mddpg_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/1274851048.py\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0mnext_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0mtarget_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m             \u001b[0mtarget_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtarget_q\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/1274851048.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, state, action)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1702\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1704\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1705\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":8}]}