import numpy as np
import random
from collections import deque, namedtuple
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

# Experience tuple for replay buffer
Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])

class ReplayBuffer:
    """Experience Replay Buffer with uniform sampling."""
    
    def __init__(self, capacity: int):
        self.buffer = deque(maxlen=capacity)
    
    def push(self, state: tuple, action: int, reward: float, next_state: tuple, done: bool):
        experience = Experience(state, action, reward, next_state, done)
        self.buffer.append(experience)
    
    def sample(self, batch_size: int):
        experiences = random.sample(self.buffer, batch_size)
        
        states = torch.FloatTensor([exp.state for exp in experiences])
        actions = torch.LongTensor([exp.action for exp in experiences])
        rewards = torch.FloatTensor([exp.reward for exp in experiences])
        next_states = torch.FloatTensor([exp.next_state for exp in experiences])
        dones = torch.FloatTensor([exp.done for exp in experiences])
        
        return states, actions, rewards, next_states, dones
    
    def __len__(self):
        return len(self.buffer)


class QNetwork(nn.Module):
    """Deep Q-Network using PyTorch.
    Adjusted to match ql.py: 3 shared layers [128,128,128], dueling with additional [128] layer.
    """
    
    def __init__(self, state_dim: int, action_dim: int, hidden_units: list = [128, 128, 128], 
                 dueling_hidden_units: list = [128],
                 activation: str = 'relu', use_dueling: bool = True):
        super(QNetwork, self).__init__()
        
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.use_dueling = use_dueling
        
        # Select activation
        if activation == 'relu':
            self.activation = nn.ReLU()
        elif activation == 'tanh':
            self.activation = nn.Tanh()
        elif activation == 'elu':
            self.activation = nn.ELU()
        
        # Build shared layers (match hidden_structure [128,128,128])
        layers = []
        input_dim = state_dim
        
        for hidden_dim in hidden_units:
            layers.append(nn.Linear(input_dim, hidden_dim))
            layers.append(self.activation)
            input_dim = hidden_dim
        
        self.shared_layers = nn.Sequential(*layers)
        
        if use_dueling:
            # Dueling architecture matching dueling_hidden_layers [128]
            value_layers = []
            adv_layers = []
            input_dim = hidden_units[-1]
            for hidden_dim in dueling_hidden_units:
                value_layers.append(nn.Linear(input_dim, hidden_dim))
                value_layers.append(self.activation)
                adv_layers.append(nn.Linear(input_dim, hidden_dim))
                adv_layers.append(self.activation)
                input_dim = hidden_dim
            
            value_layers.append(nn.Linear(input_dim, 1))
            adv_layers.append(nn.Linear(input_dim, action_dim))
            
            self.value_stream = nn.Sequential(*value_layers)
            self.advantage_stream = nn.Sequential(*adv_layers)
        else:
            # Standard DQN
            self.output_layer = nn.Linear(hidden_units[-1], action_dim)
        
        # Initialize weights
        self._initialize_weights()
    
    def _initialize_weights(self):
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_normal_(module.weight)  # glorot_normal
                nn.init.constant_(module.bias, 0.0)
    
    def forward(self, state: torch.Tensor) -> torch.Tensor:
        x = self.shared_layers(state)
        
        if self.use_dueling:
            value = self.value_stream(x)
            advantage = self.advantage_stream(x)
            q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))
        else:
            q_values = self.output_layer(x)
        
        return q_values


class DQNAgent:
    """DQN Agent for Pricing Competition.
    Adjusted to match ql.py logic where possible.
    """
    
    def __init__(self,
                 agent_id: int,
                 state_dim: int,
                 action_dim: int,
                 hidden_units: list = [128, 128, 128],
                 dueling_hidden_units: list = [128],
                 replay_capacity: int = 10000,
                 batch_size: int = 32,
                 gamma: float = 0.99,
                 epsilon: float = 1.0,
                 epsilon_decay: float = 0.95,
                 epsilon_end: float = 0.3,
                 learning_rate: float = 0.001,
                 target_update_freq: int = 1000,
                 loss_type: str = 'mse',  # 'mse' or 'huber'
                 use_double: bool = True,
                 use_dueling: bool = True,
                 gradient_clip: float = None,
                 device: str = 'cuda' if torch.cuda.is_available() else 'cpu',
                 seed: int = None):
        
        self.agent_id = agent_id
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_end = epsilon_end
        self.batch_size = batch_size
        self.device = torch.device(device)
        self.gradient_clip = gradient_clip
        self.use_double = use_double
        self.target_update_freq = target_update_freq
        self.loss_history = []
        self.epsilon_history = []
        self.train_steps = 0
        self.episodes = 0
        self.norm_fact = 1000.0  # Match NORM_FACT
        
        if seed is not None:
            random.seed(seed)
            np.random.seed(seed)
            torch.manual_seed(seed)
            if torch.cuda.is_available():
                torch.cuda.manual_seed_all(seed)
        
        # Initialize networks
        self.q_network = QNetwork(state_dim, action_dim, hidden_units, dueling_hidden_units,
                                  use_dueling=use_dueling).to(self.device)
        self.target_network = QNetwork(state_dim, action_dim, hidden_units, dueling_hidden_units,
                                       use_dueling=use_dueling).to(self.device)
        self.target_network.load_state_dict(self.q_network.state_dict())
        
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)
        
        self.memory = ReplayBuffer(replay_capacity)
        
        if loss_type == 'huber':
            self._compute_loss = F.smooth_l1_loss
        else:
            self._compute_loss = F.mse_loss
    
    def get_state_representation(self, state: tuple) -> np.ndarray:
        """Normalize state to [0,1] range."""
        return np.array([state[0] / (self.action_dim - 1), state[1] / (self.action_dim - 1)])
    
    def select_action(self, state: tuple, explore: bool = True) -> int:
        """Epsilon-greedy action selection. Match ql.py random uniform round."""
        if explore and random.random() < self.epsilon:
            return round(random.uniform(0, self.action_dim - 1))
        else:
            with torch.no_grad():
                state_vec = self.get_state_representation(state)
                state_tensor = torch.FloatTensor(state_vec).unsqueeze(0).to(self.device)
                q_values = self.q_network(state_tensor)
                return q_values.argmax().item()
    
    def remember(self, state: tuple, action: int, reward: float, next_state: tuple, done: bool):
        """Store experience in replay buffer."""
        self.memory.push(state, action, reward, next_state, done)
    
    def replay(self):
        """Perform training on batch from replay buffer.
        To match ql.py, if buffer large enough, sample batch_size.
        Note: ql.py trains on all recent tuples in shuffled batches, but here we keep standard sampling.
        For closer match, could process multiple batches or all buffer in shuffled order.
        """
        if len(self.memory) < self.batch_size:
            return None
        
        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)
        
        states = states.to(self.device)
        actions = actions.to(self.device)
        rewards = (rewards / self.norm_fact).to(self.device)  # Normalize reward
        next_states = next_states.to(self.device)
        dones = dones.to(self.device)
        
        # Compute current Q
        current_q = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)
        
        # Compute target Q
        with torch.no_grad():
            if self.use_double:
                # DDQN: actions from q_network, values from target
                next_actions = self.q_network(next_states).argmax(dim=1)
                next_q = self.target_network(next_states).gather(1, next_actions.unsqueeze(1)).squeeze(1)
            else:
                next_q = self.target_network(next_states).max(dim=1)[0]
            
            target_q = rewards + self.gamma * next_q * (1 - dones)
        
        # Compute loss
        loss = self._compute_loss(target_q, current_q)
        
        # Optimize
        self.optimizer.zero_grad()
        loss.backward()
        
        if self.gradient_clip is not None:
            torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), self.gradient_clip)
        
        self.optimizer.step()
        
        # Update target network
        self.train_steps += 1
        if self.train_steps % self.target_update_freq == 0:
            self.target_network.load_state_dict(self.q_network.state_dict())
        
        loss_value = loss.item()
        self.loss_history.append(loss_value)
        
        return loss_value
    
    def update_epsilon(self):
        """Decay exploration rate. Match ql.py decay."""
        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)
        self.epsilon_history.append(self.epsilon)
    
    def get_q_values(self, state: tuple) -> np.ndarray:
        """Get Q-values for all actions."""
        with torch.no_grad():
            state_vec = self.get_state_representation(state)
            state_tensor = torch.FloatTensor(state_vec).unsqueeze(0).to(self.device)
            q_values = self.q_network(state_tensor)
            return q_values.cpu().numpy().flatten()
    
    def save(self, filepath: str):
        """Save model."""
        checkpoint = {
            'q_network_state_dict': self.q_network.state_dict(),
            'target_network_state_dict': self.target_network.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'epsilon': self.epsilon,
            'train_steps': self.train_steps,
            'episodes': self.episodes
        }
        torch.save(checkpoint, filepath)
        print(f"Model saved to {filepath}")
    
    def load(self, filepath: str):
        """Load model."""
        checkpoint = torch.load(filepath, map_location=self.device)
        self.q_network.load_state_dict(checkpoint['q_network_state_dict'])
        self.target_network.load_state_dict(checkpoint['target_network_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.epsilon = checkpoint['epsilon']
        self.train_steps = checkpoint['train_steps']
        self.episodes = checkpoint['episodes']
        print(f"Model loaded from {filepath}")


if __name__ == "__main__":
    """Test the PyTorch DQN implementation."""
    
    print("="*80)
    print("DQN AGENT - PyTorch Version (Matched to ql.py)")
    print("="*80)
    
    # Create price grid
    n_prices = 15
    price_nash = 1.47
    price_monopoly = 2.0
    
    prices = np.linspace(
        price_nash - 0.15 * (price_monopoly - price_nash),
        price_monopoly + 0.15 * (price_monopoly - price_nash),
        n_prices
    )
    
    print(f"\nPrice Grid: [{prices[0]:.3f}, {prices[-1]:.3f}]")
    print(f"Number of Prices: {n_prices}\n")
    
    # Initialize agent with matched params
    agent = DQNAgent(
        agent_id=0,
        state_dim=2,
        action_dim=n_prices,
        loss_type='mse',  # Match MSE in ql.py
        use_double=True,
        use_dueling=True
    )
    
    # Test interaction
    print("Testing Basic Interaction:")
    print("-" * 80)
    
    state = (7, 7)
    print(f"State: {state}")
    
    state_vec = agent.get_state_representation(state)
    print(f"Normalized: {state_vec}")
    
    action = agent.select_action(state, explore=True)
    print(f"Action: {action} (price: ${prices[action]:.3f})")
    
    q_values = agent.get_q_values(state)
    print(f"Q-values: min={q_values.min():.4f}, max={q_values.max():.4f}")
    
    # Store experience
    agent.remember(state, action, 0.5, (action, 7), False)
    print(f"Buffer: {len(agent.memory)}/{agent.memory.buffer.maxlen}")
    
    # Network info
    print(f"\nNetwork Parameters: {sum(p.numel() for p in agent.q_network.parameters()):,}")
    
    print("\n" + "="*80)
    print("PyTorch DQN Ready!")
    print("="*80)
